---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "02/08/2025"
output:
  pdf_document:
    toc: yes
    toc_depth: '6'
  html_document:
    toc: yes
    toc_depth: 6
    number_sections: no
toc-title: Table of Contents
---



> <u>**P3**</u>

# Purpose

## Background

The new decade has begun with a very rocky start with the US and Iran almost at the brink of war, the Russian-Ukrainian war, Australian Bush fires, East African Locust swarms, earthquakes, devastating floods upon other things, but the COVID-19 Pandemic has slowly descending upon humanity as the deadliest event yet. In the following analysis, I make sense of how the daily deaths due to COVID-19 compares to the individual states' quickness of lock down, population size by state, and distance of case from the first 5 cases in the USA amongst other variables.


# Data Acquisition

In order to help aid in predicting the spread of this virus and just analyzing the previous path of the virus I will be using data collected by the John Hopkins Center for Systems Science and Engineering.
The data is being further sourced from WHO, CDC, ECDC, NHC, DXY, 1point3acres, Worldometers.info, BNO, the COVID Tracking Project (testing and hospitalizations), state and national government health departments, and local media reports.
The data set was found on [Kaggle](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series). Alongside this data set, a new data set was created by compiling numerous statistics specific to each US state. These statistics were curated via multiple government websites, after thorough research into possible connections between the US states and the spread of Covid-19. First we will look at the basic summary statistics to show light on the general picture of what we are working with and then moving on to more robust analysis via regression analysis. We will then use forecasting to predict the future of this pandemic. 

This pandemic is one of the most important events happening right now and the way we react and approach it will affect our lives forever. Understanding it bit by bit will at least somewhat help us in making the decisions to combat COVID-19, which is very important to our survival on this planet.


<br>


## Libraries

The following libraries were used in the analysis:
*tidyverse, lubridate, broom, oslrr, car, mctest, MPV, cvTools, gh, gitcreds, usethis, httr, httr2(binary)*
```{r setup, echo = TRUE, warning = FALSE, message = FALSE, results = "hide"}

knitr::opts_chunk$set(echo = TRUE)
require("knitr")
# Setting working directory for the project
opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Loading all necessary libraries
library(tidyverse)
library(lubridate)
library(broom)
library(olsrr)
library(car)
library(mctest)
library(MPV)
library(cvTools)
library(gh)
library(httr)       
library(gitcreds)   
library(usethis)    
suppressMessages(suppressWarnings(install.packages("httr2",type="binary")))
library(httr2)

```

<br>


## GitHub Authentication

We set up our Git credentials, verify them and add our GitHub token in order to access Covid data through the GitHub API. We store the token and any passwords locally for security reasons.
@1.
```{r, echo = TRUE, eval = FALSE}
#Setting up Git credentials
gitcreds_set()
```
@2.
```{r, echo = TRUE, eval = FALSE}
#Verifying Git credentials
gitcreds_get()
```
@3.
```{r, echo = TRUE, eval = FALSE}
#Adding Token
usethis::edit_r_environ()  # Adding token manually to .Renviron
readRenviron("~/.Renviron") # Reloading the environment
Sys.getenv("GITHUB_PAT") # Making sure it is set


#Testing Connection
usethis::gh_token_help()
response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
content(response)
```
@4.
```{r, echo = TRUE, eval = FALSE}
#Automating GitHub authentication
Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```
<br>


## Data Import

<br>

### COVID data

Downloading and cleaning the COVID-19 data sets from GitHub provided by John Hopkins University. We download all files and save them into a list, staging for processing.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
```
***Here we encountered a problem since the original GitHub repository directory has been truncated to 1,000 files, omitting 63 files. For this analysis, we will only use data from these 1,000 files.***

<br>


# Standardizing Data and Combining All Datasets

Upon further investigation, we need to rid the data frame of unnecessary observations and irrelevant variables. The original data included observations from *American Samoa*, *Diamond Princess*, *Grand Princess*, *Guam*, *Northern Mariana Islands*, *Puerto Rico*, *Virgin Islands* and *District of Columbia* **(the cruise ships which originally brought the virus to the west)**, which we are not interested in. We are only interested in the 50 United States of America. Several variables in the dataset were reviewed and determined to be unsuitable for our analysis, such as **"FIPS"**, **"Total_Test_Results"**, **"People_Hospitalized"**, **"Case_Fatality_Ratio"**, **"UID"**, **"ISO3"**, **"Testing_Rate"**, **"Hospitalization_Rate"**, **"People_Tested"**, **"Mortality_Rate"**, **"Date"**, **"Incident_Rate"**. Further, we address missing values in the active cases and recovered cases columns, adding appropriate values based on the data.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Defining the banned strings and pattern for row removal
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
# Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

# Defining the column names to remove (case-insensitive)
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

# Processing each data frame in covid_data_list
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Removing rows based on banned values in columns whose name contains "State" or "Province"
  # Identifying columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filtering out any row where at least one of the checked columns contains a banned string
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Removing unwanted columns (ignoring case)
  # We compare the lower-case version of each column name
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Replacing missing values in "Active" with the corresponding value from "Confirmed"
  # Locating the columns regardless of case
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replacing if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Replacing missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})
```

<br>


## Verifying Data Standardization Before Merging

After processing the vast quantity of files, we make sure each file consists of 50 rows of data, corresponding to the 50 states in the US, and that they don't include erroneous columns, or extra unnecessary data.
```{r, echo = TRUE, warning = FALSE, message = FALSE}
# Checking if all CSVs have the same column names:
# Getting the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Using purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Checking if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}
```


# Weather Data Transformations

A portion of our weather data is saved in 4 folders separated by year, and labeled by state in each respective folder in the working directory. We process each file by creating a new column for the state, specifying the year column, removing granular data and calculating averages in order to smooth some statistics. We make sure the data has a `State` and `Year` column, as these will serve as join keys when combining our multiple data sets.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Defining the main data directory
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data"

# Listing all year folders
year_folders <- c("2020", "2021", "2022", "2023")

# Defining function to process each file
process_weather_file <- function(file_path, year) {
  # Extracting state name from file name (without extension)
  state <- tools::file_path_sans_ext(basename(file_path))
  
  # Loading the data
  df <- read_csv(file_path, show_col_types = FALSE)
  
  # Removing unnecessary columns
  remove_cols <- c("station", "day", "min_rh", "max_rh")

  df <- df %>%
    select(-all_of(remove_cols)) %>%
    
    # Creating avg_temp column
    mutate(avg_temp = ifelse("min_temp_f" %in% names(df) & "max_temp_f" %in% names(df) &
                             !is.na(min_temp_f) & !is.na(max_temp_f),
                             (min_temp_f + max_temp_f) / 2, NA),
           
           # Creating avg_dp column
           avg_dp = ifelse("min_dewpoint_f" %in% names(df) & "max_dewpoint_f" %in% names(df) &
                           !is.na(min_dewpoint_f) & !is.na(max_dewpoint_f),
                           (min_dewpoint_f + max_dewpoint_f) / 2, NA)) %>%
    
    # Removing min/max temp and dew point columns if they exist
    select(-any_of(c("max_temp_f", "min_temp_f", "max_dewpoint_f", "min_dewpoint_f"))) %>%
    
    # Renaming columns if they exist
    rename_with(~ c("avg_precip", "avg_ws")[match(.x, c("precip_in", "avg_wind_speed_kts"))], 
                .cols = any_of(c("precip_in", "avg_wind_speed_kts"))) %>%
    
    # Adding Year column
    mutate(Year = year)

  # Computing yearly averages for numeric columns (excluding NULLs)
  yearly_avg <- df %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) %>%
    mutate(Year = year, State = state)
  
  return(yearly_avg)
}

# Processing all files in each year folder
all_data <- map_dfr(year_folders, function(year) {
  year_path <- file.path(main_dir, year)
  
  # Getting all CSV files in the folder
  csv_files <- list.files(year_path, pattern = "\\.csv$", full.names = TRUE)
  
  # Processing each file
  map_dfr(csv_files, function(file) process_weather_file(file, year))
})
# Saving final combined data set if needed
# write_csv(all_data, file.path(main_dir, "combined_weather_data.csv"))
```

<br>

# Population Data Transformations

We reshape data from wide format to long, keeping the state and area columns the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
# Reading the data
df <- read.csv("./Data/Population.csv")

# Reshaping data from wide to long format for years
population_df <- df %>%
  pivot_longer(cols = 2:5, names_to = "year", values_to = "pop_d") %>%
  mutate(year = sub("^X", "", year))  
# Removing "X" from year values (common when reading year column in csv files, R is reading s a string instead of integer)
```

# Population Age Statistics Data Transformations

From the age statistics by population data, we reshape data from wide format to long, keeping the state column the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year, and age percentage columns. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, eval = TRUE, warning = FALSE, message = FALSE}
# Reading the data
df <- read.csv("./Data/infant_elder_avg_age.csv", check.names = FALSE)

# Renaming columns
df <- df %>%
  rename(state = NAME,
         `2020` = POPEST2020_CIV,
         `2021` = POPEST2021_CIV,
         `2022` = POPEST2022_CIV,
         `2023` = POPEST2023_CIV)

# Reshaping from wide to long format for years
df_long <- df %>%
  pivot_longer(cols = `2020`:`2023`, names_to = "year", values_to = "population") 

# Reshaping 'AGE' from long to wide, using the actual records: "Infant percentage" and "elder percentage"
population2_df <- df_long %>%
  pivot_wider(names_from = AGE, values_from = population) %>%
  rename(pop_infant = `Infant percentage`, pop_elder = `elder percentage`)  # Use exact column names

# Selecting only required columns
population2_df <- df_final %>%
  select(state, year, pop_infant, pop_elder)
```
<br>


# Airport Traffic Data Transformations

The airport traffic data, or enplanements per state from 2020-2023 is separated into 4 files corresponding to each respective year. We change the state abbreviation back to the full state name in order to ensure `State` column is consistent across all data frames. The enplanement data has been recorded from multiple states, each with multiple airports, so we group the data by state and aggregate it accordingly.
```{r, warning = FALSE, message = FALSE}
# Defining the path to the data folder
data_path <- "./Data/airport_traffic"

# Listing years corresponding to the files
years <- 2020:2023

# Creating a named vector for state abbreviation-to-full-name mapping
state_abbr <- setNames(state.name, state.abb)

# Defining function to process each file
process_file <- function(year) {
  file <- file.path(data_path, paste0(year, ".csv"))
  # Renaming columns, converting state abbreviations to full names, and dropping rows where state conversion failed 
  df <- read.csv(file, check.names = FALSE) %>%
    rename(state = STATE, enplanements = Enplanements) %>%
    mutate(state = state_abbr[state], year = year) %>%
    drop_na(state)

  return(df)
}

# Reading and processing all files
df_list <- lapply(years, process_file)

# Combining all data over the years into one data set
traffic_df <- bind_rows(df_list) %>%
  group_by(state, year) %>%
  summarise(enplanements = sum(enplanements, na.rm = TRUE), .groups = "drop")
```
<br>

# Supplemental Data Import


```{r, warning = FALSE, message = FALSE}
# Reading the data
income_df <- read.csv("./Data/poverty_income/Income.csv", check.names = FALSE)
regressors_df <- read.csv("./Data/Regressors.csv", check.names = FALSE)
weather_df <- read.csv("./Data/Weather Data/combined_weather_data.csv", check.names = FALSE)

# Binding all rows of clean covid data set list
covid_df <- bind_rows(covid_data_list_clean)
```


```{r, warning = FALSE, message = FALSE}
# Defining the directory path
data_dir <- "./Data/Average State Precipitation"

# Getting list of CSV files
csv_files <- list.files(path = data_dir, pattern = "\\.csv$", full.names = TRUE)

# Initializing empty list to store processed data frames
df_list <- list()

# Processing each file
for (file in csv_files) {
  
  # Reading CSV, skipping first 4 rows and without predefined headers
  df <- read_csv(file, skip = 4, col_names = FALSE, col_types = cols(.default = "c"))
  

  # Manually setting column names
  colnames(df) <- c("Year", "Value")
  
  # Converting Year to character
  df$Year <- as.character(df$Year)
  
  # Extracting Month (MM) from Year column (YYYYMM format)
  df <- df %>%
    mutate(Month = substr(Year, 5, 6)) %>%
    relocate(Month, .after = Year)
  
  # Keeping only the YYYY part in the Year column
  df$Year <- substr(df$Year, 1, 4)
  
  # Extracting State name from file name
  state_name <- tools::file_path_sans_ext(basename(file))
  
  # Adding State column at the first position
  df <- df %>%
    mutate(State = state_name) %>%
    relocate(State, .before = Year)
  
  # Storing cleaned data frame
  df_list[[length(df_list) + 1]] <- df
}

precip_m_df <- bind_rows(df_list)
```




```{r, warning = FALSE, message = FALSE}
# Defining the directory path
data_dir <- "./Data/Average State Temperatures"

# Getting list of CSV files
csv_files <- list.files(path = data_dir, pattern = "\\.csv$", full.names = TRUE)

# Initializing empty list to store processed data frames
df_list <- list()

# Processing each file
for (file in csv_files) {
  
  # Reading CSV, skipping first 4 rows and without predefined headers
  df <- read_csv(file, skip = 4, col_names = FALSE, col_types = cols(.default = "c"))

  # Manually setting column names
  colnames(df) <- c("Year", "Value")
  
  # Converting Year to character
  df$Year <- as.character(df$Year)
  
  # Extracting Month (MM) from Year column (YYYYMM format)
  df <- df %>%
    mutate(Month = substr(Year, 5, 6)) %>%
    relocate(Month, .after = Year)
  
  # Keeping only the YYYY part in the Year column
  df$Year <- substr(df$Year, 1, 4)
  
  # Extracting State name from file name
  state_name <- tools::file_path_sans_ext(basename(file))
  
  # Adding State column at the first position
  df <- df %>%
    mutate(State = state_name) %>%
    relocate(State, .before = Year)
  
  # Storing cleaned data frame
  df_list[[length(df_list) + 1]] <- df
}

temp_m_df <- bind_rows(df_list)
```




```{r, warning = FALSE, message = FALSE}
# 
covid_df <- as.data.frame(covid_df)
population_df <- as.data.frame(population_df)
population2_df <- as.data.frame(population2_df)
traffic_df <- as.data.frame(traffic_df)
precip_m_df <- as.data.frame(precip_m_df)
temp_m_df <-  as.data.frame(temp_m_df)

covid_df <- covid_df %>%
  select("State" = names(covid_df)[1], "Date" = names(covid_df)[3], names(covid_df)[4], names(covid_df)[5], names(covid_df)[6], names(covid_df)[7], names(covid_df)[8], names(covid_df)[9])

head(covid_df)


population_df <- population_df %>%
  select("State" = names(population_df)[1], "Year" = names(population_df)[2], names(population_df)[3])

head(population_df)


weather_df <- weather_df %>%
  select(names(weather_df)[7], names(weather_df)[6], names(weather_df)[1], names(weather_df)[2], names(weather_df)[3], names(weather_df)[4], names(weather_df)[5])

head(weather_df)


population2_df <- population2_df %>%
  select("State" = names(population2_df)[1], "Year" = names(population2_df)[2], names(population2_df)[3], names(population2_df)[4])

head(population2_df)


traffic_df <- traffic_df %>%
  select("State" = names(traffic_df)[1], "Year" = names(traffic_df)[2], names(traffic_df)[3])

head(traffic_df)


income_df <- income_df %>%
  select("State" = names(income_df)[1], "Year" = names(income_df)[2], names(income_df)[3], names(income_df)[4])

head(income_df)

temp_m_df <- temp_m_df %>%
  mutate(State = as.character(State),     # Convert numeric → integer
         Year = as.integer(Year),
         Month = as.integer(Month),
         avg_temp_m = as.numeric(Value)) %>%
  select(State, Year, Month, avg_temp_m)
head(temp_m_df)

precip_m_df <- precip_m_df %>%
  mutate(State = as.character(State),     # Convert numeric → integer
         Year = as.integer(Year),
         Month = as.integer(Month),
         avg_precip_m = as.numeric(Value)) %>%
  select(State, Year, Month, avg_precip_m)
```




```{r, warning = FALSE, message = FALSE}

population_df <- population_df %>%
  mutate(Year = as.integer(Year)     # Convert numeric → integer
         )
# Check column types
sapply(population_df, class)


population2_df <- population2_df %>%
  mutate(Year = as.integer(Year)     # Convert numeric → integer
         )
# Check column types
sapply(population2_df, class)


income_df <- income_df %>%
  mutate(Poverty_Percent = as.numeric(Poverty_Percent),     # Convert numeric → integer
         Median_Household_Income = as.integer(gsub(",", "",Median_Household_Income))
         )
# Check column types
sapply(income_df, class)


covid_df <- covid_df %>%
  mutate(Confirmed = as.integer(Confirmed),     # Convert numeric → integer
         Deaths = as.integer(Deaths),
         Recovered = as.integer(Recovered),
         Active = as.integer(Active)
         )
# Check column types
sapply(covid_df, class)

```



```{r, warning = FALSE, message = FALSE}
head(covid_df)
head(population_df)
head(weather_df)
head(population2_df)
head(traffic_df)
head(income_df)
head(regressors_df)
head(precip_m_df)
head(temp_m_df)
```



```{r, warning = FALSE, message = FALSE}
covid_df <- covid_df %>%
  mutate(Year = as.integer(format(Date, "%Y")),
         Month = as.integer(format(Date, "%m")))%>%
  select(names(covid_df)[1], Year, Month, everything())

head(covid_df)
```


```{r, warning = FALSE, message = FALSE}
# Left join using composite key (State + Year)
rolling_avg <- covid_df %>%
  left_join(temp_m_df, by = c("State","Year","Month"))%>%
  left_join(precip_m_df, by = c("State", "Year","Month"))

Descriptive_data_df <- population_df %>%
  left_join(weather_df, by = c("State", "Year")) %>%
  left_join(population2_df, by = c("State", "Year")) %>%
  left_join(traffic_df, by = c("State", "Year")) %>%
  left_join(income_df, by = c("State", "Year")) %>%
  left_join(regressors_df, by = c("State", "Year"))
  

head(Descriptive_data_df)
head(rolling_avg)


Data_df <- Descriptive_data_df %>%
  left_join(rolling_avg, by =c("State","Year"))

head(Data_df)
```


# Plotting variables
```{r, warning = FALSE, message = FALSE}
Data_df <- Data_df %>%
  select(names(Data_df)[1], names(Data_df)[2], names(Data_df)[19], names(Data_df)[20], names(Data_df)[21], names(Data_df)[22], everything(),-Active)

head(Data_df)

summary(Data_df)
str(Data_df)
```


```{r, warning = FALSE, message = FALSE}
plot(Data_df$Date, Data_df$avg_precip, type = "l", col = "blue",
     xlab = "Date", ylab = "Rain (In.)", main = "Precipitaion")

plot(Data_df$Date, Data_df$avg_ws, type = "l", col = "blue",
     xlab = "Date", ylab = "Speed (kts.)", main = "Wind Speed")

plot(Data_df$Date, Data_df$avg_temp, type = "l", col = "blue",
     xlab = "Date", ylab = "Temp *C", main = "Temperature")

plot(Data_df$Date, Data_df$avg_dp, type = "l", col = "blue",
     xlab = "Date", ylab = "Dew Point *C", main = "Dew point")

plot(Data_df$Date, Data_df$Confirmed, type = "l", col = "blue",
     xlab = "Date", ylab = "Confirmed Cases", main = "Confirmed")

plot(Data_df$Date, Data_df$Deaths, type = "l", col = "blue",
     xlab = "Date", ylab = "Deaths", main = "Deaths")

plot(Data_df$Date, Data_df$Recovered, type = "l", col = "blue",
     xlab = "Date", ylab = "Recovered", main = "Recovered")

plot(Data_df$Date, Data_df$avg_temp_m, type = "l", col = "blue",
     xlab = "Date", ylab = "Temperature *C", main = "Average Monthly Temperature")

plot(Data_df$Date, Data_df$avg_precip_m, type = "l", col = "blue",
     xlab = "Date", ylab = "Rainfall In.", main = "Average Monthly Precipitation")


```


```{r, warning = FALSE, message = FALSE}
acf(Data_df$avg_precip, main = "avg_Y_precip Autocorrelation Plot")
acf(Data_df$avg_dp, main = "avg_dp Autocorrelation Plot")
acf(Data_df$avg_ws, main = "avg_ws Autocorrelation Plot")
acf(Data_df$avg_temp, main = "avg_Y_temp Autocorrelation Plot")
acf(Data_df$Confirmed, main = "Confirmed Autocorrelation Plot")
acf(Data_df$Deaths, main = "Deaths Autocorrelation Plot")
acf(Data_df$Recovered, main = "Recovered Autocorrelation Plot")
acf(Data_df$avg_precip_m, main = "avg_M_precip Autocorrelation Plot")
acf(Data_df$avg_temp_m, main = "avg_M_temp Autocorrelation Plot")
```


```{r, warning = FALSE, message = FALSE}
library(tseries)
adf.test(Data_df$avg_temp_m)  # Check stationarity
adf.test(Data_df$avg_precip_m)
adf.test(Data_df$Confirmed)
```

## Correlation matrix
```{r, warning = FALSE, message = FALSE}
cor_matrix <- cor(Data_df, method = "pearson") 
```


```{r, warning = FALSE, message = FALSE}
plot(Data_df$Date, Data_df$avg_temp, type="l", col="blue", main="Temperature Over Time")
plot(Data_df$Date, Data_df$Confirmed, type="l", col="red", main="COVID Cases Over Time")
```

```{r, warning = FALSE, message = FALSE}
library(ggplot2)
library(forecast)

ts_data <- ts(Data_df$avg_temp_m, start=c(2020,1), frequency=12)
stl_decomp <- stl(ts_data, s.window="periodic")

autoplot(stl_decomp)
```

```{r, warning = FALSE, message = FALSE}
ts_data <- ts(Data_df$avg_precip_m, start=c(2020,1), frequency=12)
stl_decomp <- stl(ts_data, s.window="periodic")

autoplot(stl_decomp)
```

```{r, warning = FALSE, message = FALSE}

```

```{r, warning = FALSE, message = FALSE}

```






## Variables List
Median Household Income	Days_Until_Lockdown	Distance_To_Nearest_Epicenter	Airport_Presence	Beach_Presence	Number_Of_State_Parks

$y_1$ *(Dependent Variable) = Daily Deaths Reported*

$y_2$ *(Dependent Variable) = Daily Cases Reported*

$y_3$ *(Dependent Variable) = Daily Recovered Reported*

$x_1$ *(Independent Variable) = Average Precipitation per Year*

$x_2$ *(Independent Variable) = Average Wind Speed per Year*

$x_3$ *(Independent Variable) = Average Relative Humidity per Year*

$x_4$ *(Independent Variable) = Average Dew Point per Year*

$x_5$ *(Independent Variable) = Average Temperature per Year*

$x_6$ *(Independent Variable) = Total Population per Year*

$x_7$ *(Independent Variable) = Total Population Density (p/mi^2 | person per square mile) per Year*

$x_{8}$ *(Independent Variable) = Elder population 65 >= %*

$x_{9}$ *(Independent Variable) = Infant population 5 <= %*

$x_8$ *(Independent Variable) = Number of days between first US case and lock down/shelter-in-place order*

$x_9$ *(Independent Variable) = Distance from capital to the closest of the 5 states with the first case*

$x_{10}$ *(Independent Variable) = If the state has an airport*

$x_{15}$ *(Independent Variable) = If the state has a beach*

$x_{16}$ *(Independent Variable) = Number of state parks*

$x_6$ *(Independent Variable) = State Expenditures for the Fiscal Year*

$x_{17}$ *(Independent Variable) = Airport Traffic*

$x_{20}$ *(Independent Variable) = Poverty percentage*

$x_{21}$ *(Independent Variable) = Unemployment percentage*

$x_{22}$ *(Independent Variable) = Median household income*

$x_{23}$ *(Independent Variable) = deaths/cases*


# Sources

Main Covid19 Data
1. https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us

Precipitation & Temperature Data
2. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
3. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
4. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
5. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
6. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022

State Expenditures Data
7. https://nasbo.org/commerce/datasets

Enplanement Data
8. https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger

Any and all Climate Data
9. https://mesonet.agron.iastate.edu/request/daily.phtml?

Poverty and Income Data
10. https://www.census.gov/programs-surveys/saipe/data/datasets.html

Unemployment Rates Data
11. https://www.bls.gov/lau/tables.htm#stateaa



