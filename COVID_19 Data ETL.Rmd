---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "2025-02-08"
output: pdf_document
---


```{r setup, warning = FALSE, message = FALSE, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)
require("knitr")

opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Setting working directory for the project

library(tidyverse)  # Includes dplyr, ggplot2, tidyr, readr, etc.
library(lubridate)  # For handling date conversions
library(broom)      # For tidying model outputs
library(olsrr)      # For regression diagnostics
library(car)        # Regression diagnostics
library(mctest)     # Multicollinearity test
library(MPV)        # Regression modeling
library(cvTools)    # Cross-validation


# Loading all necessary libraries

#install.packages("gh")
library(gh)  #using Github API to access datasets rather than storing them on local machine




```




# Data Import


## Setting up GitHub Authentication


```{r, warning = FALSE, message = FALSE}

#install.packages("httr")
#install.packages("gitcreds")  # For managing GitHub credentials
#install.packages("usethis")   # For GitHub authentication setup
library(httr)
library(gitcreds)
library(usethis)

```



```{r, warning = FALSE, message = FALSE}
gitcreds_set()

```


```{r, warning = FALSE, message = FALSE}
gitcreds_get()

```

```{r, warning = FALSE, message = FALSE}
#Adding Token
#usethis::edit_r_environ()  # Adding token manually to .Renviron
#readRenviron("~/.Renviron") # Reloading the environment
#Sys.getenv("GITHUB_PAT") # Making sure it is set


#Testing Connection
#usethis::gh_token_help()
#response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
#content(response)

```


```{r, warning = FALSE, message = FALSE}

Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```





## Initial Data Import

```{r, warning = FALSE, message = FALSE}
#install.packages("httr2",type="binary")
library(httr2)
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# List the CSV files
head(csv_files)

```






```{r, warning = FALSE, message = FALSE}
# Base URL for files
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
#Problem here since, the original GitHub repository directory has been truncated to 1,000 files, ommitting 63 files

# Combine the data if needed (e.g., bind them into one large data frame)
#combined_data <- do.call(rbind, covid_data_list)

# Check the combined data
#head(combined_data)

```

### Confirming Data Import 

```{r, warning = FALSE, message = FALSE}
summary(covid_data_list)

```

# Standardizing Data and Combining All Datasets

```{r, warning = FALSE, message = FALSE}
# --- Defining the banned strings and pattern for row removal ---
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
# Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

# --- Defining the column names to remove (case-insensitive) ---
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

# --- Process each data frame in covid_data_list ---
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Step 1: Remove rows based on banned values in columns whose name contains "State" or "Province"
  # Identify columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filter out any row where at least one of the checked columns contains a banned string.
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Step 2: Remove unwanted columns (ignoring case)
  # We compare the lower-case version of each column name.
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Step 3: Replace missing values in "Active" with the corresponding value from "Confirmed"
  # Locate the columns regardless of case.
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replace if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Step 4: Replace missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})




#covid_data_list

```



```{r, warning = FALSE, message = FALSE}
head(covid_data_list_clean[[1]],50)
#head(dataset_list[[1]])
```

#Making sure the data is standardized before combining

```{r, warning = FALSE, message = FALSE}
# Check if all CSVs have the same column names:
# Get the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Use purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Check if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}




```






```{r, warning = FALSE, message = FALSE}
data <- bind_rows(covid_data_list_clean)


```


```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```







# Sources



Precipitation & Temperature Data
1. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
2. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
3. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
4. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
5. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022





