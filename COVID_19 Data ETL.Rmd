---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "02/08/2025"
output:
  pdf_document:
    toc: yes
    toc_depth: '6'
  html_document:
    toc: yes
    toc_depth: 6
    number_sections: no
toc-title: Table of Contents
---



> <u>**P3**</u>

# Purpose

## Background

The new decade has begun with a very rocky start with the US and Iran almost at the brink of war, the Russian-Ukrainian war, Australian Bush fires, East African Locust swarms, earthquakes, devastating floods upon other things, but the COVID-19 Pandemic has slowly descending upon humanity as the deadliest event yet. In the following analysis, I make sense of how the daily deaths due to COVID-19 compares to the individual states' quickness of lock down, population size by state, and distance of case from the first 5 cases in the USA amongst other variables.


# Data Acquisition

In order to help aid in predicting the spread of this virus and just analyzing the previous path of the virus I will be using data collected by the John Hopkins Center for Systems Science and Engineering.
The data is being further sourced from WHO, CDC, ECDC, NHC, DXY, 1point3acres, Worldometers.info, BNO, the COVID Tracking Project (testing and hospitalizations), state and national government health departments, and local media reports.
The data set was found on [Kaggle](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series). Alongside this data set, a new data set was created by compiling numerous statistics specific to each US state. These statistics were curated via multiple government websites, after thorough research into possible connections between the US states and the spread of Covid-19. First we will look at the basic summary statistics to show light on the general picture of what we are working with and then moving on to more robust analysis via regression analysis. We will then use forecasting to predict the future of this pandemic. 

This pandemic is one of the most important events happening right now and the way we react and approach it will affect our lives forever. Understanding it bit by bit will at least somewhat help us in making the decisions to combat COVID-19, which is very important to our survival on this planet.


<br>


## Libraries

The following libraries were used in the analysis:
*tidyverse, lubridate, broom, oslrr, car, mctest, MPV, cvTools, gh, gitcreds, usethis, httr, httr2(binary)*
```{r setup, echo = TRUE, warning = FALSE, message = FALSE, results = "hide"}

knitr::opts_chunk$set(echo = TRUE)
require("knitr")
# Setting working directory for the project
opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Loading all necessary libraries
library(tidyverse)
library(lubridate)
library(broom)
library(olsrr)
library(car)
library(mctest)
library(MPV)
library(cvTools)
library(gh)
library(httr)       
library(gitcreds)   
library(usethis)    
suppressMessages(suppressWarnings(install.packages("httr2",type="binary")))
library(httr2)

```

<br>


## GitHub Authentication

We set up our Git credentials, verify them and add our GitHub token in order to access Covid data through the GitHub API. We store the token and any passwords locally for security reasons.
@1.
```{r, echo = TRUE, eval = FALSE}
#Setting up Git credentials
gitcreds_set()
```
@2.
```{r, echo = TRUE, eval = FALSE}
#Verifying Git credentials
gitcreds_get()
```
@3.
```{r, echo = TRUE, eval = FALSE}
#Adding Token
usethis::edit_r_environ()  # Adding token manually to .Renviron
readRenviron("~/.Renviron") # Reloading the environment
Sys.getenv("GITHUB_PAT") # Making sure it is set


#Testing Connection
usethis::gh_token_help()
response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
content(response)
```
@4.
```{r, echo = TRUE, eval = FALSE}
#Automating GitHub authentication
Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```
<br>


## Data Import

<br>

### COVID data

Downloading and cleaning the COVID-19 data sets from GitHub provided by John Hopkins University. We download all files and save them into a list, staging for processing.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
```
***Here we encountered a problem since the original GitHub repository directory has been truncated to 1,000 files, omitting 63 files. For this analysis, we will only use data from these 1,000 files.***

<br>


# Standardizing Data and Combining All Datasets

Upon further investigation, we need to rid the data frame of unnecessary observations and irrelevant variables. The original data included observations from *American Samoa*, *Diamond Princess*, *Grand Princess*, *Guam*, *Northern Mariana Islands*, *Puerto Rico*, *Virgin Islands* and *District of Columbia* **(the cruise ships which originally brought the virus to the west)**, which we are not interested in. We are only interested in the 50 United States of America. Several variables in the dataset were reviewed and determined to be unsuitable for our analysis, such as **"FIPS"**, **"Total_Test_Results"**, **"People_Hospitalized"**, **"Case_Fatality_Ratio"**, **"UID"**, **"ISO3"**, **"Testing_Rate"**, **"Hospitalization_Rate"**, **"People_Tested"**, **"Mortality_Rate"**, **"Date"**, **"Incident_Rate"**. Further, we address missing values in the active cases and recovered cases columns, adding appropriate values based on the data.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Defining the banned strings and pattern for row removal
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
# Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

# Defining the column names to remove (case-insensitive)
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

# Processing each data frame in covid_data_list
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Removing rows based on banned values in columns whose name contains "State" or "Province"
  # Identifying columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filtering out any row where at least one of the checked columns contains a banned string
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Removing unwanted columns (ignoring case)
  # We compare the lower-case version of each column name
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Replacing missing values in "Active" with the corresponding value from "Confirmed"
  # Locating the columns regardless of case
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replacing if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Replacing missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})
```

<br>


## Verifying Data Standardization Before Merging

After processing the vast quantity of files, we make sure each file consists of 50 rows of data, corresponding to the 50 states in the US, and that they don't include erroneous columns, or extra unnecessary data.
```{r, echo = TRUE, warning = FALSE, message = FALSE}
# Checking if all CSVs have the same column names:
# Getting the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Using purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Checking if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}
```


# Weather Data Transformations

A portion of our weather data is saved in 4 folders separated by year, and labeled by state in each respective folder in the working directory. We process each file by creating a new column for the state, specifying the year column, removing granular data and calculating averages in order to smooth some statistics. We make sure the data has a `State` and `Year` column, as these will serve as join keys when combining our multiple data sets.
```{r, echo = TRUE, eval= FALSE, results = "hide", warning = FALSE, message = FALSE}
# Defining the main data directory
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data"

# Listing all year folders
year_folders <- c("2020", "2021", "2022", "2023")

# Defining function to process each file
process_weather_file <- function(file_path, year) {
  # Extracting state name from file name (without extension)
  state <- tools::file_path_sans_ext(basename(file_path))
  
  # Loading the data
  df <- read_csv(file_path, show_col_types = FALSE)
  
  # Checking if the "date" column exists, otherwise return empty
  if (!"day" %in% names(df)) return(NULL)
  
  # Removing unnecessary columns
  remove_cols <- c("station", "min_rh", "max_rh")
  df <- df %>%
    select(-all_of(remove_cols)) %>%
    
    # Convert date column to Date format & extract month
    mutate(date = ymd(day),  # Assuming the date is in YYYY-MM-DD format
           Month = month(day)) %>%
    
    # Creating avg_temp column
    mutate(avg_temp = ifelse("min_temp_f" %in% names(df) & "max_temp_f" %in% names(df) &
                             !is.na(min_temp_f) & !is.na(max_temp_f),
                             (min_temp_f + max_temp_f) / 2, NA),
           
           # Creating avg_dp column
           avg_dp = ifelse("min_dewpoint_f" %in% names(df) & "max_dewpoint_f" %in% names(df) &
                           !is.na(min_dewpoint_f) & !is.na(max_dewpoint_f),
                           (min_dewpoint_f + max_dewpoint_f) / 2, NA)) %>%
    
    # Removing min/max temp and dew point columns if they exist
    select(-any_of(c("max_temp_f", "min_temp_f", "max_dewpoint_f", "min_dewpoint_f"))) %>%
    
    # Renaming columns if they exist
    rename_with(~ c("avg_precip", "avg_ws")[match(.x, c("precip_in", "avg_wind_speed_kts"))], 
                .cols = any_of(c("precip_in", "avg_wind_speed_kts"))) %>%
    
    # Adding Year and State column
    mutate(Year = year, State = state)

  # Computing monthly averages
  monthly_avg <- df %>%
    group_by(State, Year, Month) %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop")

  # Computing yearly averages
  yearly_avg <- df %>%
    group_by(State, Year) %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
    rename_with(~ paste0("avg_m_", .x), .cols = -c(State, Year))  # Renaming yearly avg cols

  # Merging yearly averages into monthly dataset
  final_data <- left_join(monthly_avg, yearly_avg, by = c("State", "Year"))

  return(final_data)
}

# Processing all files in each year folder
all_data <- map_dfr(year_folders, function(year) {
  year_path <- file.path(main_dir, year)
  
  # Getting all CSV files in the folder
  csv_files <- list.files(year_path, pattern = "\\.csv$", full.names = TRUE)
  
  # Processing each file
  map_dfr(csv_files, function(file) process_weather_file(file, year))
})
# Saving final combined data set to cut down on computing time
#write_csv(all_data, file.path(main_dir, "combined_weather_data.csv"))
```

<br>

# Population Data Transformations

We reshape data from wide format to long, keeping the state and area columns the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Reading the data
df <- read.csv("./Data/Population.csv")

# Reshaping data from wide to long format for years
population_df <- df %>%
  pivot_longer(cols = 2:5, names_to = "year", values_to = "pop_d") %>%
  mutate(year = sub("^X", "", year))  
# Removing "X" from year values (common when reading year column in csv files, R is reading s a string instead of integer)
```

# Population Age Statistics Data Transformations

From the age statistics by population data, we reshape data from wide format to long, keeping the state column the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year, and age percentage columns. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Reading the data
df <- read.csv("./Data/infant_elder_avg_age.csv", check.names = FALSE)

# Renaming columns
df <- df %>%
  rename(state = NAME,
         `2020` = POPEST2020_CIV,
         `2021` = POPEST2021_CIV,
         `2022` = POPEST2022_CIV,
         `2023` = POPEST2023_CIV)

# Reshaping from wide to long format for years
df_long <- df %>%
  pivot_longer(cols = `2020`:`2023`, names_to = "year", values_to = "population") 

# Reshaping 'AGE' from long to wide, using the actual records: "Infant percentage" and "elder percentage"
population2_df <- df_long %>%
  pivot_wider(names_from = AGE, values_from = population) %>%
  rename(pop_infant = `Infant percentage`, pop_elder = `elder percentage`)  # Use exact column names

# Selecting only required columns
population2_df <- df_final %>%
  select(state, year, pop_infant, pop_elder)
```
<br>


# Airport Traffic Data Transformations

The airport traffic data, or enplanements per state from 2020-2023 is separated into 4 files corresponding to each respective year. We change the state abbreviation back to the full state name in order to ensure `State` column is consistent across all data frames. The enplanement data has been recorded from multiple states, each with multiple airports, so we group the data by state and aggregate it accordingly.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Defining the path to the data folder
data_path <- "./Data/airport_traffic"

# Listing years corresponding to the files
years <- 2020:2023

# Creating a named vector for state abbreviation-to-full-name mapping
state_abbr <- setNames(state.name, state.abb)

# Defining function to process each file
process_file <- function(year) {
  file <- file.path(data_path, paste0(year, ".csv"))
  # Renaming columns, converting state abbreviations to full names, and dropping rows where state conversion failed 
  df <- read.csv(file, check.names = FALSE) %>%
    rename(state = STATE, enplanements = Enplanements) %>%
    mutate(state = state_abbr[state], year = year) %>%
    drop_na(state)

  return(df)
}

# Reading and processing all files
df_list <- lapply(years, process_file)

# Combining all data over the years into one data set
traffic_df <- bind_rows(df_list) %>%
  group_by(state, year) %>%
  summarise(enplanements = sum(enplanements, na.rm = TRUE), .groups = "drop")
```
<br>

# Additional Weather Data Import

Average state precipitation data and temperature data are already separated by `State`, the `Date` column is further decomposed into `Year`, ensuring a common join key, and by `Month`. By decomposing `Date` into granular components, trend, seasonality, and residual noise,  applying rolling averages to smooth short-term fluctuations, we can better understand the underlying patterns that impact our dependent variables, enabling more accurate forecasting and strategic decision-making.   

<br>

# Supplemental Data Import

Importing a few more relative data sets that may help support our hypothesis/analysis.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Reading the data
income_df <- read.csv("./Data/poverty_income/Income.csv", check.names = FALSE)
regressors_df <- read.csv("./Data/Regressors.csv", check.names = FALSE)
weather_df <- read.csv("./Data/Weather Data/combined_weather_data.csv", check.names = FALSE)

# Binding all rows of clean covid data set list
covid_df <- bind_rows(covid_data_list_clean)
```

<br>

# Data Wrangling

We ensure that all data sets are in `data.frame` format rather than `data.table` or any other mixed structures.
```{r, echo = TRUE, warning = FALSE, message = FALSE}
# Converting data tables to data frames
covid_df <- as.data.frame(covid_df)
population_df <- as.data.frame(population_df)
population2_df <- as.data.frame(population2_df)
traffic_df <- as.data.frame(traffic_df)

# Making sure all data are in data.frame format
objects <- list(covid_df = covid_df, population_df = population_df, population2_df = population2_df, traffic_df = traffic_df, weather_df = weather_df, income_df = income_df, regressors_df = regressors_df)
sapply(objects, is.data.frame)
```

<br>



Verifying that columns align properly, and remove any extraneous data to prepare them for consolidation. When necessary, we also address any inconsistencies to streamline the process.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Renaming and making sure case is constant throughout all data sets for join keys
# Decomposing Date into Year and Month
# Reordering columns as needed
# Making sure variable classes are correct
# Selecting only required columns
covid_df <- covid_df %>%
  mutate(
    Date = as.POSIXct(Last_Update),
    across(c(Confirmed, Deaths, Recovered, Active), as.integer),
    Year = as.integer(year(Date)), 
    Month = as.integer(month(Date))
  ) %>%
  rename(State = names(covid_df)[1]) %>%
  relocate(Year, Month, .after = 1) %>%
  select(State, Year, Month, Date, names(covid_df)[4:8])

# Renaming and making sure case is constant throughout all data sets for join keys
# Making sure variable classes are correct
# Selecting only required columns
population_df <- population_df %>%
  mutate(Year = as.integer(.data[[names(population_df)[2]]])) %>%
  select("State" = names(population_df)[1], Year, names(population_df)[3])

# Selecting only required columns
# Reordering columns as needed
weather_df <- weather_df %>%
  select(State, Year, Month, everything())

# Renaming and making sure case is constant throughout all data sets for join keys
# Selecting only required columns
# Making sure variable classes are correct
population2_df <- population2_df %>%
  mutate(Year = as.integer(.data[[names(population2_df)[2]]])) %>%
  select("State" = names(population2_df)[1], Year, names(population2_df)[3], names(population2_df)[4])

# Renaming and making sure case is constant throughout all data sets for join keys
traffic_df <- traffic_df %>%
  select("State" = names(traffic_df)[1], "Year" = names(traffic_df)[2], names(traffic_df)[3])

# Renaming and making sure case is constant throughout all data sets for join keys
# Making sure variable classes are correct
income_df <- income_df %>%
  mutate(Poverty_Percent = as.numeric(Poverty_Percent), 
         Median_Household_Income = as.integer(gsub(",", "",Median_Household_Income))) %>%
  select("State" = names(income_df)[1], "Year" = names(income_df)[2], names(income_df)[3], names(income_df)[4])
```
<br>


# Joining all data frames


We visualize the variable class type for all variables in our data frames before joining them and double-check that the join key maintains a consistent data type across all data frames to facilitate a smooth and accurate join.
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
dfs <- list(
  covid_df = covid_df,
  population_df = population_df,
  population2_df = population2_df,
  income_df = income_df,
  weather_df = weather_df,
  traffic_df = traffic_df,
  regressors_df = regressors_df
#  precip_m_df = precip_m_df,
#  temp_m_df = temp_m_df
)

# Creating a structured table
df_classes <- map_dfr(dfs, function(df) {
  tibble(
    Variable = names(df),
    Class = sapply(df, function(x) paste(class(x), collapse = ", "))
  )
}, .id = "DataFrame")

# Heatmap visualization
ggplot(df_classes, aes(x = DataFrame, y = Variable, fill = Class)) +
  geom_tile(color = "white") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(title = "Variable Classes Across Data Frames", x = "Data Frame", y = "Variable") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
*Fig. 1 Shows the join key, `State` + `Year`, maintains a consistent data type across all data frames.*
<br>


# Left Join

First use **left_join** by `State`,`Year`, and `Month`, then we join the rest of the data together before joining to original set by `State` and `Year`.
```{r, echo = TRUE, results = "hide", warning = FALSE, message = FALSE}
# Left join using composite key (State + Year)
Data_df1 <- population_df %>%
  left_join(weather_df, by = c("State", "Year")) %>%
  left_join(population2_df, by = c("State", "Year")) %>%
  left_join(traffic_df, by = c("State", "Year")) %>%
  left_join(income_df, by = c("State", "Year")) %>%
  left_join(regressors_df, by = c("State", "Year"))

Data_df1 <- Data_df1 %>%
  select(State, Year, Month, everything())

rolling_avg <- covid_df %>%
  left_join(Data_df1, by = c("State", "Year","Month"))
```
<br>

# Rearrange
We rearrange the columns, grouping and bringing `State`, `Year`, and `Date` to the front and keeping `Confirmed`, `Deaths` and `Recovered` to the back.
```{r, echo = TRUE, warning = FALSE, message = FALSE}
Data <- rolling_avg %>%
  mutate(avg_ws_y = avg_m_avg_ws, avg_rh_y = avg_m_avg_rh, avg_temp_y = avg_m_avg_temp, avg_dp_y = avg_m_avg_dp, avg_precip_y = avg_m_avg_precip)

Data <- Data %>%
  select(names(Data)[1:8], names(Data)[10:15], names(Data)[22:36])

main <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"
# Saving final combined data set to cut down on computing time
#write_csv(Data, file.path(main, "Data.csv"))
```

# Checking for `NULL` values.
```{r, echo = TRUE, warning = FALSE, message = FALSE,}
sum(is.na(Data))
```
<br>

# Exploratory Analysis

# Plotting variables

```{r, warning = FALSE, message = FALSE}
plot(Data$Date, Data$avg_precip, type = "l", col = "blue",
     xlab = "Date", ylab = "Rain (In.)", main = "Precipitaion")

plot(Data$Date, Data$avg_ws, type = "l", col = "blue",
     xlab = "Date", ylab = "Speed (kts.)", main = "Wind Speed")

plot(Data$Date, Data$avg_temp, type = "l", col = "blue",
     xlab = "Date", ylab = "Temp *C", main = "Temperature")

plot(Data$Date, Data$avg_dp, type = "l", col = "blue",
     xlab = "Date", ylab = "Dew Point *C", main = "Dew point")

plot(Data$Date, Data$Confirmed, type = "l", col = "blue",
     xlab = "Date", ylab = "Confirmed Cases", main = "Confirmed")

plot(Data$Date, Data$Deaths, type = "l", col = "blue",
     xlab = "Date", ylab = "Deaths", main = "Deaths")

plot(Data$Date, Data$Recovered, type = "l", col = "blue",
     xlab = "Date", ylab = "Recovered", main = "Recovered")

```

```{r warning = FALSE, message = FALSE, fig.cap="Fig. 4",echo=FALSE}
library(corrplot)

Data_num <- Data[, sapply(Data, is.numeric)]
data_cor <- cor(Data_num)

corrplot(data_cor, method = "circle", tl.cex = 0.6, 
         order = "hclust", number.cex = 0.7, diag = FALSE)
```


*Violin plots*
```{r}
# Select numeric, integer, and double variables, excluding "Year", "Month", "Long_", and "Lat"
df_filtered <- Data %>%
  select(where(is.numeric)) %>%
  select(-c(Year, Month, Long_, Lat))

# Convert to long format for ggplot
df_long_all <- pivot_longer(df_filtered, cols = everything(), names_to = "Variable", values_to = "Value")

# Define groups based on the provided list
variable_groups <- list(
  group_2 = c("avg_ws", "avg_rh", "avg_temp", "avg_dp", "avg_precip"),
  group_3 = c("avg_ws_y","avg_rh_y", "avg_temp_y", "avg_dp_y","avg_precip_y"),
  group_5 = c("Airport_Presence", "Beach_Presence"),
  group_7 = c("Confirmed"),
  group_8 = c("Deaths"),
  group_10 = c("pop_d"),
  group_11 = c("avg_precip"),
  group_12 = c("avg_ws"),
  group_13 = c("avg_rh"),
  group_14 = c("avg_temp"),
  group_15 = c("avg_dp"),
  group_16 = c("pop_infant"),
  group_17 = c("pop_elder"),
  group_18 = c("enplanements"),
  group_19 = c("Poverty_Percent"),
  group_20 = c("Median_Household_Income"),
  group_21 = c("Days_Until_Lockdown"),
  group_22 = c("Distance_To_Nearest_Epicenter"),
  group_23 = c("Airport_Presence"),
  group_24 = c("Beach_Presence"),
  group_25 = c("Number_Of_State_Parks"),
  group_26 = c("avg_temp_y"),
  group_27 = c("avg_precip_y")
)

create_violin_plot <- function(var_list, title) {
  df_subset <- df_long_all %>% filter(Variable %in% var_list)

  ggplot(df_subset, aes(x = Variable, y = Value, fill = Variable)) +
    geom_violin(trim = FALSE, alpha = 0.6) +  
    geom_boxplot(width = 0.1, outlier.shape = NA, aes(fill = Variable), color = "black", alpha = 0.2) +  
    stat_summary(fun.min = min, fun.max = max, geom = "errorbar", width = 0.3, color = "black") +  
    scale_fill_brewer(palette = "Paired") +  
    theme_minimal() +
    labs(title = title, x = "Variable", y = "Values") +
    theme(legend.position = "none")
}
# Generate violin plots for each group
violin_plots <- lapply(names(variable_groups), function(group_name) {
  create_violin_plot(variable_groups[[group_name]], paste("Violin Plot for", group_name))
})
# Print each violin plot
for (p in violin_plots) {
  print(p)
}
```

## Correlation matrix

```{r}
# Convert 'Year' to factor and encode 'Month' cyclically
Data <- Data %>%
  mutate(Year = as.factor(Year),  # Convert Year to factor
         Month_sin = sin(2 * pi * Month / 12), 
         Month_cos = cos(2 * pi * Month / 12)) %>%
  select(-Year, -Month)  # Remove raw Year & Month
# Keep only numeric columns and exclude Lat & Long_
numeric_data <- Data %>%
  select(where(is.numeric)) %>%
  select(-Lat, -Long_)  # Explicitly remove Lat and Long_

# Compute Pearson correlation
pearson_cor <- cor(numeric_data, use = "pairwise.complete.obs", method = "pearson")

# Compute Spearman correlation
spearman_cor <- cor(numeric_data, use = "pairwise.complete.obs", method = "spearman")

# Plot both correlation matrices
par(mfrow = c(1,2))  # Set up side-by-side plots

corrplot(pearson_cor, method = "color", tl.cex = 0.7, title = "Pearson Correlation", mar=c(0,0,1,0))
corrplot(spearman_cor, method = "color", tl.cex = 0.7, title = "Spearman Correlation", mar=c(0,0,1,0))

par(mfrow = c(1,1))  # Reset plotting layout
```



```{r, warning = FALSE, message = FALSE}
# Define variable pairs to check for cross-correlation
ccf_pairs <- list(
  c("Confirmed", "avg_temp"),
  c("Confirmed", "avg_precip"),
  c("Confirmed", "avg_ws"),
  c("Confirmed", "avg_dp"),
  c("Confirmed", "avg_rh"),
  c("Confirmed", "enplanements"),
  c("Deaths", "avg_temp"),
  c("Deaths", "avg_precip"),
  c("Deaths", "avg_ws"),
  c("Deaths", "avg_dp"),
  c("Deaths", "avg_rh"),
  c("Deaths", "enplanements"),
  c("Confirmed", "avg_temp_y"),
  c("Confirmed", "avg_precip_y"),
  c("Confirmed", "avg_ws_y"),
  c("Confirmed", "avg_dp_y"),
  c("Confirmed", "avg_rh_y"),
  c("Deaths", "avg_temp_y"),
  c("Deaths", "avg_precip_y"),
  c("Deaths", "avg_ws_y"),
  c("Deaths", "avg_dp_y"),
  c("Deaths", "avg_rh_y")
)
# Loop through each pair and plot CCF
par(mfrow = c(2,2))  # Set up 2x2 plot layout

for (pair in ccf_pairs) {
  ccf(numeric_data[[pair[1]]], numeric_data[[pair[2]]], lag.max = 90, 
      main = paste("CCF:", pair[1], "vs.", pair[2]), ylab = "Correlation")
}
par(mfrow = c(1,1))  # Reset plot layout
```

```{r, warning = FALSE, message = FALSE}
install.packages("mgcv")
library(mgcv)

# Fit a GAM model to see nonlinear relationships
gam_model <- gam(Confirmed ~ s(avg_temp) + s(avg_rh), data = Data)

# Check significance of nonlinear effects
summary(gam_model)

# Visualize the smooth functions
plot(gam_model, pages = 1)


```

```{r, warning = FALSE, message = FALSE}
lm_model <- lm(Confirmed ~ avg_temp + avg_rh + enplanements, data = Data)
summary(lm_model)
```

```{r, warning = FALSE, message = FALSE}
library(forecast)

# Convert Confirmed Cases into a time series object
ts_confirmed <- ts(Data$Confirmed, start = c(2020, 1), frequency = 12)  # Monthly

# Decompose time series
decomposed <- decompose(ts_confirmed, type = "multiplicative")

# Plot decomposition
autoplot(decomposed)
```

```{r, warning = FALSE, message = FALSE}
# Histogram for temperature
ggplot(Data, aes(x = avg_temp)) +
  geom_histogram(binwidth = 2, fill = "blue", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Temperature", x = "Temperature (Â°C)", y = "Count")

# Histogram for humidity
ggplot(Data, aes(x = avg_rh)) +
  geom_histogram(binwidth = 5, fill = "red", color = "black", alpha = 0.7) +
  theme_minimal() +
  labs(title = "Distribution of Relative Humidity", x = "Humidity (%)", y = "Count")
```

```{r, warning = FALSE, message = FALSE}
# Fit a new GAM model with an interaction term
gam_model_interaction <- gam(Confirmed ~ s(avg_temp) + s(avg_rh) + ti(avg_temp, avg_rh), data = Data)

# Print model summary
summary(gam_model_interaction)

# Plot smooth functions including interaction
plot(gam_model_interaction, pages = 1, shade = TRUE)
```

```{r, warning = FALSE, message = FALSE}
AIC(gam_model, gam_model_interaction)
```

```{r, warning = FALSE, message = FALSE}
#suppressMessages(suppressWarnings(install.packages("visreg",type="binary")))
library(visreg)

# 3D Surface Plot for Interaction Effect
visreg2d(gam_model_interaction, "avg_temp", "avg_rh", plot.type = "persp", cond = list())

```

```{r, warning = FALSE, message = FALSE}
# Fit an enhanced GAM model with mobility and policy effects
gam_model_mobility <- gam(
  Confirmed ~ s(avg_temp) + s(avg_rh) + ti(avg_temp, avg_rh) + 
               s(enplanements) + s(Days_Until_Lockdown) + s(Poverty_Percent),
  data = Data
)

# Print summary of new model
summary(gam_model_mobility)
```

```{r, warning = FALSE, message = FALSE}
AIC(gam_model_interaction, gam_model_mobility)
```

```{r, warning = FALSE, message = FALSE}
par(mfrow = c(1,2))  # Arrange plots in 1 row, 2 columns

plot(gam_model_mobility, select = 4, shade = TRUE, main = "Effect of Air Travel on COVID Cases")
plot(gam_model_mobility, select = 5, shade = TRUE, main = "Effect of Lockdowns on COVID Cases")

```

```{r, warning = FALSE, message = FALSE}
# Predict next 30 days of COVID cases
forecast_cases <- predict(gam_model_mobility, newdata = Data, type = "response")

# Compare actual vs. predicted cases
Data$predicted_cases <- forecast_cases

ggplot(Data, aes(x = Confirmed, y = predicted_cases)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Actual vs. Predicted COVID Cases (with Mobility & Policy)", x = "Actual Cases", y = "Predicted Cases")

```

```{r, warning = FALSE, message = FALSE}
Data$log_Confirmed <- log1p(Data$Confirmed)  # log(1 + x) prevents log(0) issues

gam_model_log <- gam(
  log_Confirmed ~ s(avg_temp) + s(avg_rh) + ti(avg_temp, avg_rh) + 
                  s(enplanements) + s(Days_Until_Lockdown) + s(Poverty_Percent),
  data = Data
)

# Predict with log model and back-transform
Data$predicted_log_cases <- exp(predict(gam_model_log, newdata = Data)) - 1

# Plot again
ggplot(Data, aes(x = Confirmed, y = predicted_log_cases)) +
  geom_point(color = "blue", alpha = 0.5) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  theme_minimal() +
  labs(title = "Log-Transformed Model: Actual vs. Predicted Cases", x = "Actual Cases", y = "Predicted Cases")

```

```{r, warning = FALSE, message = FALSE}
gam_model_mobility2 <- gam(
  Confirmed ~ s(avg_temp) + s(avg_rh) + ti(avg_temp, avg_rh) + 
               s(enplanements) + s(Days_Until_Lockdown) + ti(enplanements, Days_Until_Lockdown),
  data = Data
)


AIC(gam_model_mobility2, gam_model_mobility)
```
```{r}
install.packages("caret")
library(caret)
varImp(gam_model_mobility2)
```


```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```

```{r}


```






## Variables List
Median Household Income	Days_Until_Lockdown	Distance_To_Nearest_Epicenter	Airport_Presence	Beach_Presence	Number_Of_State_Parks

$y_1$ *(Dependent Variable) = Daily Deaths Reported*

$y_2$ *(Dependent Variable) = Daily Cases Reported*

$y_3$ *(Dependent Variable) = Daily Recovered Reported*

$x_1$ *(Independent Variable) = Average Precipitation per Year*

$x_2$ *(Independent Variable) = Average Wind Speed per Year*

$x_3$ *(Independent Variable) = Average Relative Humidity per Year*

$x_4$ *(Independent Variable) = Average Dew Point per Year*

$x_5$ *(Independent Variable) = Average Temperature per Year*

$x_6$ *(Independent Variable) = Total Population per Year*

$x_7$ *(Independent Variable) = Total Population Density (p/mi^2 | person per square mile) per Year*

$x_{8}$ *(Independent Variable) = Elder population 65 >= %*

$x_{9}$ *(Independent Variable) = Infant population 5 <= %*

$x_8$ *(Independent Variable) = Number of days between first US case and lock down/shelter-in-place order*

$x_9$ *(Independent Variable) = Distance from capital to the closest of the 5 states with the first case*

$x_{10}$ *(Independent Variable) = If the state has an airport*

$x_{15}$ *(Independent Variable) = If the state has a beach*

$x_{16}$ *(Independent Variable) = Number of state parks*

$x_6$ *(Independent Variable) = State Expenditures for the Fiscal Year*

$x_{17}$ *(Independent Variable) = Airport Traffic*

$x_{20}$ *(Independent Variable) = Poverty percentage*

$x_{21}$ *(Independent Variable) = Unemployment percentage*

$x_{22}$ *(Independent Variable) = Median household income*

$x_{23}$ *(Independent Variable) = deaths/cases*


# Sources

Main Covid19 Data
1. https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us

Precipitation & Temperature Data
2. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
3. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
4. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
5. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
6. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022

State Expenditures Data
7. https://nasbo.org/commerce/datasets

Enplanement Data
8. https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger

Any and all Climate Data
9. https://mesonet.agron.iastate.edu/request/daily.phtml?

Poverty and Income Data
10. https://www.census.gov/programs-surveys/saipe/data/datasets.html

Unemployment Rates Data
11. https://www.bls.gov/lau/tables.htm#stateaa



