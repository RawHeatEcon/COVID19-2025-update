---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "2025-02-08"
output: pdf_document
---


```{r setup, warning = FALSE, message = FALSE, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)
require("knitr")

opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Setting working directory for the project

library(tidyverse)  # Includes dplyr, ggplot2, tidyr, readr, etc.
library(lubridate)  # For handling date conversions
library(broom)      # For tidying model outputs
library(olsrr)      # For regression diagnostics
library(car)        # Regression diagnostics
library(mctest)     # Multicollinearity test
library(MPV)        # Regression modeling
library(cvTools)    # Cross-validation


# Loading all necessary libraries

#install.packages("gh")
library(gh)  #using Github API to access datasets rather than storing them on local machine




```




# Data Import


## Setting up GitHub Authentication


```{r, warning = FALSE, message = FALSE}

#install.packages("httr")
#install.packages("gitcreds")  # For managing GitHub credentials
#install.packages("usethis")   # For GitHub authentication setup
library(httr)
library(gitcreds)
library(usethis)

```



```{r, warning = FALSE, message = FALSE}
gitcreds_set()

```


```{r, warning = FALSE, message = FALSE}
gitcreds_get()

```

```{r, warning = FALSE, message = FALSE}
#Adding Token
#usethis::edit_r_environ()  # Adding token manually to .Renviron
#readRenviron("~/.Renviron") # Reloading the environment
#Sys.getenv("GITHUB_PAT") # Making sure it is set


#Testing Connection
#usethis::gh_token_help()
#response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
#content(response)

```


```{r, warning = FALSE, message = FALSE}

Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```





## Initial Data Import

```{r, warning = FALSE, message = FALSE}
#install.packages("httr2",type="binary")
library(httr2)
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# List the CSV files
head(csv_files)

```






```{r, warning = FALSE, message = FALSE}
# Base URL for files
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
#Problem here since, the original GitHub repository directory has been truncated to 1,000 files, ommitting 63 files

# Combine the data if needed (e.g., bind them into one large data frame)
#combined_data <- do.call(rbind, covid_data_list)

# Check the combined data
#head(combined_data)

```

### Confirming Data Import 

```{r, warning = FALSE, message = FALSE}
summary(covid_data_list)

```

# Standardizing Data and Combining All Datasets

```{r, warning = FALSE, message = FALSE}
# --- Defining the banned strings and pattern for row removal ---
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
# Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

# --- Defining the column names to remove (case-insensitive) ---
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

# --- Process each data frame in covid_data_list ---
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Step 1: Remove rows based on banned values in columns whose name contains "State" or "Province"
  # Identify columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filter out any row where at least one of the checked columns contains a banned string.
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Step 2: Remove unwanted columns (ignoring case)
  # We compare the lower-case version of each column name.
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Step 3: Replace missing values in "Active" with the corresponding value from "Confirmed"
  # Locate the columns regardless of case.
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replace if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Step 4: Replace missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})




#covid_data_list

```



```{r, warning = FALSE, message = FALSE}
head(covid_data_list_clean[[1]],50)
#head(dataset_list[[1]])
```

#Making sure the data is standardized before combining

```{r, warning = FALSE, message = FALSE}
# Check if all CSVs have the same column names:
# Get the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Use purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Check if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}




```






```{r, warning = FALSE, message = FALSE}
data <- bind_rows(covid_data_list_clean)


```


```{r, warning = FALSE, message = FALSE}
library(readr)
library(purrr)

# Defining the main data directory for the weather data
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data"

# Listing all year folders
year_folders <- c("2020", "2021", "2022", "2023")

# Function to process each file
process_weather_file <- function(file_path, year) {
  # Extract state name from file name (without extension)
  state <- tools::file_path_sans_ext(basename(file_path))
  
  # Read the CSV file
  df <- read_csv(file_path, show_col_types = FALSE) %>%
    # Remove unnecessary columns
    select(-station, -day, -min_rh, -max_rh) %>%
    
    # Create avg_temp column
    mutate(avg_temp = ifelse(is.na(min_temp_f) | is.na(max_temp_f), NA, (min_temp_f + max_temp_f) / 2),
           
           # Create avg_dp column
           avg_dp = ifelse(is.na(min_dewpoint_f) | is.na(max_dewpoint_f), NA, (min_dewpoint_f + max_dewpoint_f) / 2)) %>%
    
    # Remove min/max temp and dewpoint columns
    select(-max_temp_f, -min_temp_f, -max_dewpoint_f, -min_dewpoint_f) %>%
    
    # Rename columns
    rename(avg_precip = precip_in, avg_ws = avg_wind_speed_kts) %>%
    
    # Add Year column
    mutate(Year = year)
  
  # Compute yearly averages for numeric columns (excluding NULLs)
  yearly_avg <- df %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) %>%
    mutate(Year = year, State = state)
  
  return(yearly_avg)
}

# Process all files in each year folder
all_data <- map_dfr(year_folders, function(year) {
  year_path <- file.path(main_dir, year)
  
  # Get all CSV files in the folder
  csv_files <- list.files(year_path, pattern = "\\.csv$", full.names = TRUE)
  
  # Process each file
  map_dfr(csv_files, function(file) process_weather_file(file, year))
})

# Save final combined dataset
write_csv(all_data, file.path(main_dir, "combined_weather_data.csv"))
```


```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```



## Variables List
Median Household Income	Days_Until_Lockdown	Distance_To_Nearest_Epicenter	Airport_Presence	Beach_Presence	Number_Of_State_Parks

READY $y_1$ *(Dependent Variable) = Daily Deaths Reported*

READY $y_2$ *(Dependent Variable) = Daily Cases Reported*

READY $y_3$ *(Dependent Variable) = Daily Recovered Reported*

READY $x_1$ *(Independent Variable) = Average Precipitation per Year*

READY $x_2$ *(Independent Variable) = Average Wind Speed per Year*

READY $x_3$ *(Independent Variable) = Average Relative Humidity per Year*

READY $x_4$ *(Independent Variable) = Average Dew Point per Year*

READY $x_5$ *(Independent Variable) = Average Temperature per Year*

READY $x_6$ *(Independent Variable) = Total Population per Year*

READY $x_7$ *(Independent Variable) = Total Population Density (p/mi^2 | person per square mile) per Year*

READY $x_{8}$ *(Independent Variable) = Elder population 65 >= %*

READY $x_{9}$ *(Independent Variable) = Infant population 5 <= %*

READY $x_8$ *(Independent Variable) = Number of days between first US case and lock down/shelter-in-place order*

READY $x_9$ *(Independent Variable) = Distance from capital to the closest of the 5 states with the first case*

READY $x_{10}$ *(Independent Variable) = If the state has an airport*

READY $x_{15}$ *(Independent Variable) = If the state has a beach*

READY $x_{16}$ *(Independent Variable) = Number of state parks*

READY $x_6$ *(Independent Variable) = State Expenditures for the Fiscal Year*

READY $x_{17}$ *(Independent Variable) = Airport Traffic*

READY $x_{20}$ *(Independent Variable) = Poverty percentage*

READY $x_{21}$ *(Independent Variable) = Unemployment percentage*

READY $x_{22}$ *(Independent Variable) = Median household income*

READY $x_{23}$ *(Independent Variable) = deaths/cases*


# Sources

Main Covid19 Data
1. https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us

Precipitation & Temperature Data
2. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
3. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
4. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
5. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
6. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022

State Expenditures Data
7. https://nasbo.org/commerce/datasets

Enplanement Data
8. https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger

Any and all Climate Data
9. https://mesonet.agron.iastate.edu/request/daily.phtml?

Poverty and Income Data
10. https://www.census.gov/programs-surveys/saipe/data/datasets.html

Unemployment Rates Data
11. https://www.bls.gov/lau/tables.htm#stateaa



