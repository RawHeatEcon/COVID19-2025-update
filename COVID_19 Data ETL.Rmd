---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "2025-02-08"
output: pdf_document
---


```{r setup, warning = FALSE, message = FALSE, include = FALSE}

knitr::opts_chunk$set(echo = TRUE)
require("knitr")
# Setting working directory for the project
opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Loading all necessary libraries

library(tidyverse)  # Includes dplyr, ggplot2, tidyr, readr, etc.
library(lubridate)  # For handling date conversions
library(broom)      # For tidying model outputs
library(olsrr)      # For regression diagnostics
library(car)        # Regression diagnostics
library(mctest)     # Multicollinearity test
library(MPV)        # Regression modeling
library(cvTools)    # Cross-validation
library(gh)         # using Github API to access datasets rather than storing them on local machine
library(httr)       #
library(gitcreds)   #
library(usethis)    #
#install.packages("httr2",type="binary")
library(httr2)      #
library(readr)
library(purrr)


```




# Data Import


## Setting up GitHub Authentication


```{r, warning = FALSE, message = FALSE}


```



```{r, warning = FALSE, message = FALSE}
gitcreds_set()

```


```{r, warning = FALSE, message = FALSE}
gitcreds_get()

```

```{r, warning = FALSE, message = FALSE}
#Adding Token
#usethis::edit_r_environ()  # Adding token manually to .Renviron
#readRenviron("~/.Renviron") # Reloading the environment
#Sys.getenv("GITHUB_PAT") # Making sure it is set


#Testing Connection
#usethis::gh_token_help()
#response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
#content(response)

```


```{r, warning = FALSE, message = FALSE}

Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```





## Initial Data Import

```{r, warning = FALSE, message = FALSE}
## Preliminary GitHub testing
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# List the CSV files
head(csv_files)

```






```{r, warning = FALSE, message = FALSE}
# Base URL for files
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
#Problem here since, the original GitHub repository directory has been truncated to 1,000 files, omitting 63 files

#combined_data <- do.call(rbind, covid_data_list)

# Check the combined data
#head(combined_data)

```

### Confirming Data Import 

```{r, warning = FALSE, message = FALSE}
summary(covid_data_list)

```

# Standardizing Data and Combining All Datasets

```{r, warning = FALSE, message = FALSE}
#Defining the banned strings and pattern for row removal
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
#Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

#Defining the column names to remove (case-insensitive)
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

#Process each data frame in covid_data_list
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Step 1: Remove rows based on banned values in columns whose name contains "State" or "Province"
  # Identify columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filter out any row where at least one of the checked columns contains a banned string.
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Step 2: Remove unwanted columns (ignoring case)
  # We compare the lower-case version of each column name.
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Step 3: Replace missing values in "Active" with the corresponding value from "Confirmed"
  # Locate the columns regardless of case.
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replace if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Step 4: Replace missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})




#covid_data_list

```



```{r, warning = FALSE, message = FALSE}
head(covid_data_list_clean[[1]],50)
#head(dataset_list[[1]])
```

#Making sure the data is standardized before combining

```{r, warning = FALSE, message = FALSE}
# Check if all CSVs have the same column names:
# Get the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Use purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Check if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}

```






```{r, warning = FALSE, message = FALSE}
data <- bind_rows(covid_data_list_clean)


```


```{r, warning = FALSE, message = FALSE}
# Defining the main data directory
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data"

# Listing all year folders
year_folders <- c("2020", "2021", "2022", "2023")

# Function to process each file
process_weather_file <- function(file_path, year) {
  # Extract state name from file name (without extension)
  state <- tools::file_path_sans_ext(basename(file_path))
  
  # Read the CSV file
  df <- read_csv(file_path, show_col_types = FALSE)
  
  # Check if columns exist before attempting to remove them
  remove_cols <- c("station", "day", "min_rh", "max_rh")
  remove_cols <- remove_cols[remove_cols %in% colnames(df)]  # Keep only existing columns
  
  df <- df %>%
    select(-all_of(remove_cols)) %>%
    
    # Create avg_temp column
    mutate(avg_temp = ifelse("min_temp_f" %in% names(df) & "max_temp_f" %in% names(df) &
                             !is.na(min_temp_f) & !is.na(max_temp_f),
                             (min_temp_f + max_temp_f) / 2, NA),
           
           # Create avg_dp column
           avg_dp = ifelse("min_dewpoint_f" %in% names(df) & "max_dewpoint_f" %in% names(df) &
                           !is.na(min_dewpoint_f) & !is.na(max_dewpoint_f),
                           (min_dewpoint_f + max_dewpoint_f) / 2, NA)) %>%
    
    # Remove min/max temp and dewpoint columns if they exist
    select(-any_of(c("max_temp_f", "min_temp_f", "max_dewpoint_f", "min_dewpoint_f"))) %>%
    
    # Rename columns if they exist
    rename_with(~ c("avg_precip", "avg_ws")[match(.x, c("precip_in", "avg_wind_speed_kts"))], 
                .cols = any_of(c("precip_in", "avg_wind_speed_kts"))) %>%
    
    # Add Year column
    mutate(Year = year)

  # Compute yearly averages for numeric columns (excluding NULLs)
  yearly_avg <- df %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE))) %>%
    mutate(Year = year, State = state)
  
  return(yearly_avg)
}

# Process all files in each year folder
all_data <- map_dfr(year_folders, function(year) {
  year_path <- file.path(main_dir, year)
  
  # Get all CSV files in the folder
  csv_files <- list.files(year_path, pattern = "\\.csv$", full.names = TRUE)
  
  # Process each file
  map_dfr(csv_files, function(file) process_weather_file(file, year))
})

# Save final combined dataset
write_csv(all_data, file.path(main_dir, "combined_weather_data.csv"))
```

## Kentucky Data set is missing relative humidity values**
#Fixed


#Population Data Transformations

```{r, warning = FALSE, message = FALSE}

# Reading the data
df <- read.csv("./Data/Population.csv")

# Reshaping data from wide to long format
df_long <- df %>%
  pivot_longer(cols = 2:5, names_to = "year", values_to = "pop_d") %>%
  mutate(year = sub("^X", "", year))  # Removes "X" from year values


print(df_long)
```


```{r, warning = FALSE, message = FALSE}

df <- read.csv("./Data/infant_elder_avg_age.csv", check.names = FALSE)

# Rename columns
df <- df %>%
  rename(state = NAME,
         `2020` = POPEST2020_CIV,
         `2021` = POPEST2021_CIV,
         `2022` = POPEST2022_CIV,
         `2023` = POPEST2023_CIV)

# Reshape from wide to long format for years
df_long <- df %>%
  pivot_longer(cols = `2020`:`2023`, names_to = "year", values_to = "population") 

# Reshape 'AGE' from long to wide, using the actual records: "Infant percentage" and "elder percentage"
df_final <- df_long %>%
  pivot_wider(names_from = AGE, values_from = population) %>%
  rename(pop_infant = `Infant percentage`, pop_elder = `elder percentage`)  # Use exact column names

# Select only required columns
df_final <- df_final %>%
  select(state, year, pop_infant, pop_elder)

# Display the transformed data
print(df_final)


```


```{r, warning = FALSE, message = FALSE}

# Define the path to the data folder
data_path <- "./Data/airport_traffic"

# List of years corresponding to the files
years <- 2020:2023

# Create a named vector for state abbreviation-to-full-name mapping
state_abbr <- setNames(state.name, state.abb)

# Function to process each file
process_file <- function(year) {
  file <- file.path(data_path, paste0(year, ".csv"))
  
  df <- read.csv(file, check.names = FALSE) %>%
    rename(state = STATE, enplanements = Enplanements) %>%  # Rename columns
    mutate(state = state_abbr[state], year = year) %>%  # Convert state abbreviations to full names
    drop_na(state)  # Drop rows where state conversion failed

  return(df)
}

# Read and process all files
df_list <- lapply(years, process_file)

# Combine all years into one dataset
df_combined <- bind_rows(df_list) %>%
  group_by(state, year) %>%
  summarise(enplanements = sum(enplanements, na.rm = TRUE), .groups = "drop")

# Display the transformed dataset
print(df_combined)

```



```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```


```{r, warning = FALSE, message = FALSE}

```

```{r, warning = FALSE, message = FALSE}

```


## Variables List
Median Household Income	Days_Until_Lockdown	Distance_To_Nearest_Epicenter	Airport_Presence	Beach_Presence	Number_Of_State_Parks

READY $y_1$ *(Dependent Variable) = Daily Deaths Reported*

READY $y_2$ *(Dependent Variable) = Daily Cases Reported*

READY $y_3$ *(Dependent Variable) = Daily Recovered Reported*

READY $x_1$ *(Independent Variable) = Average Precipitation per Year*

READY $x_2$ *(Independent Variable) = Average Wind Speed per Year*

READY $x_3$ *(Independent Variable) = Average Relative Humidity per Year*

READY $x_4$ *(Independent Variable) = Average Dew Point per Year*

READY $x_5$ *(Independent Variable) = Average Temperature per Year*

READY $x_6$ *(Independent Variable) = Total Population per Year*

READY $x_7$ *(Independent Variable) = Total Population Density (p/mi^2 | person per square mile) per Year*

READY $x_{8}$ *(Independent Variable) = Elder population 65 >= %*

READY $x_{9}$ *(Independent Variable) = Infant population 5 <= %*

READY $x_8$ *(Independent Variable) = Number of days between first US case and lock down/shelter-in-place order*

READY $x_9$ *(Independent Variable) = Distance from capital to the closest of the 5 states with the first case*

READY $x_{10}$ *(Independent Variable) = If the state has an airport*

READY $x_{15}$ *(Independent Variable) = If the state has a beach*

READY $x_{16}$ *(Independent Variable) = Number of state parks*

READY $x_6$ *(Independent Variable) = State Expenditures for the Fiscal Year*

READY $x_{17}$ *(Independent Variable) = Airport Traffic*

READY $x_{20}$ *(Independent Variable) = Poverty percentage*

READY $x_{21}$ *(Independent Variable) = Unemployment percentage*

READY $x_{22}$ *(Independent Variable) = Median household income*

READY $x_{23}$ *(Independent Variable) = deaths/cases*


# Sources

Main Covid19 Data
1. https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us

Precipitation & Temperature Data
2. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
3. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
4. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
5. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
6. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022

State Expenditures Data
7. https://nasbo.org/commerce/datasets

Enplanement Data
8. https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger

Any and all Climate Data
9. https://mesonet.agron.iastate.edu/request/daily.phtml?

Poverty and Income Data
10. https://www.census.gov/programs-surveys/saipe/data/datasets.html

Unemployment Rates Data
11. https://www.bls.gov/lau/tables.htm#stateaa



