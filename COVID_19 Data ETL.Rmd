---
title: "COVID-19 Data ETL"
author: "Rohit Kumar"
date: "02/08/2025"
output:
  pdf_document:
    toc: yes
    toc_depth: '6'
  html_document:
    toc: yes
    toc_depth: 6
    number_sections: no
toc-title: Table of Contents
---



> <u>**P3**</u>

# Purpose

## Background

The new decade has begun with a very rocky start with the US and Iran almost at the brink of war, the Russian-Ukrainian war, Australian Bush fires, East African Locust swarms, earthquakes, devastating floods upon other things, but the COVID-19 Pandemic had slowly descended upon humanity as the deadliest event yet. In the following analysis, I make sense of how the daily deaths due to COVID-19 compares to the individual states' quickness of lock down, population size by state, and distance of case from the first 5 cases in the USA amongst other variables.


# Data Acquisition

In order to help aid in predicting the spread of this virus and just analyzing the previous path of the virus I will be using data collected by the John Hopkins Center for Systems Science and Engineering.
The data is being further sourced from WHO, CDC, ECDC, NHC, DXY, 1point3acres, Worldometers.info, BNO, the COVID Tracking Project (testing and hospitalizations), state and national government health departments, and local media reports.
The data set was found on [Kaggle](https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_time_series) with parent repository located on GitHub. Alongside this data set, a new data set was created by compiling numerous statistics specific to each US state. These statistics were curated via multiple government websites, after thorough research into possible connections between the US states and the spread of Covid-19. First we will look at the basic summary statistics to show light on the general picture of what we are working with and then moving on to more robust analysis via regression analysis. We will then use forecasting to predict the future of this pandemic. 

This pandemic is one of the most important events happening right now and the way we react and approach it will affect our lives forever. Understanding it bit by bit will at least somewhat help us in making the decisions to combat COVID-19, which is very important to our survival on this planet.


<br>


## Libraries

The following libraries were used in the analysis:
```{r setup, echo = TRUE, warning = FALSE, message = FALSE, results = "hide"}

require("knitr")
# Setting working directory for the project
opts_knit$set(root.dir = "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project")

# Loading all necessary libraries
library(tidyverse)
library(lubridate)
library(broom)
library(olsrr)
library(car)
library(mctest)
library(MPV)
library(cvTools)
library(gh)
library(httr)       
library(gitcreds)   
library(usethis)    
suppressMessages(suppressWarnings(install.packages("httr2",type="binary")))
library(httr2)
library(corrplot)
library(MASS)
library(caret)
library(visreg)
library(forecast)
library(mgcv)
library(xgboost)

```

<br>


## GitHub Authentication

We set up our Git credentials, verify them and add our GitHub token in order to access Covid data through the GitHub API. We store the token and any passwords locally for security reasons.
@1.
```{r, echo = TRUE, eval = FALSE}
#Setting up Git credentials
gitcreds_set()
```
@2.
```{r, echo = TRUE, eval = FALSE}
#Verifying Git credentials
gitcreds_get()
```
@3.
```{r, echo = TRUE, eval = FALSE}
#Adding Token
usethis::edit_r_environ()  # Adding token manually to .Renviron
readRenviron("~/.Renviron") # Reloading the environment
Sys.getenv("GITHUB_PAT") # Making sure it is set

#Testing Connection
usethis::gh_token_help()
response <- GET("https://api.github.com/user", authenticate("RawHeatEcon", Sys.getenv("GITHUB_PAT")))
content(response)
```
@4.
```{r, echo = TRUE, eval = FALSE}
#Automating GitHub authentication
Sys.setenv(GITHUB_PAT = gitcreds_get()$password)
```
<br>


## Data Import

<br>

### COVID data

Downloading and cleaning the COVID-19 data sets from GitHub provided by John Hopkins University. We download all files and save them into a list, staging for processing.
```{r, echo = TRUE, eval = FALSE}
# GitHub repository details
repo <- "CSSEGISandData/COVID-19"
path <- "csse_covid_19_data/csse_covid_19_daily_reports_us"
base_url <- "https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_daily_reports_us/"

# Fetching all files in the directory using GitHub API
file_list <- gh("GET /repos/{owner}/{repo}/contents/{path}", 
                owner = "CSSEGISandData", repo = "COVID-19", path = path)

# Extracting the file names and filtering for CSV files
csv_files <- sapply(file_list, function(x) x$name)
csv_files <- csv_files[grepl("\\.csv$", csv_files)]

# Initializing an empty list to store the data
covid_data_list <- list()

# Looping through the files and reading them in batches
for (file in csv_files) {
  # Constructing the raw file URL
  file_url <- paste0(base_url, file)
  
  # Downloading the file and reading it into R
  response <- httr::GET(file_url)
  
  if (httr::status_code(response) == 200) {
    # Reading CSV content into R
    covid_data <- read_csv(httr::content(response, "text"))
    
    # Storing the data (will transform it if needed here)
    covid_data_list[[file]] <- covid_data
  } else {
    cat("Skipping missing file:", file, "\n")
  }
}
```
***Here we encountered a problem since the original GitHub repository directory has been truncated to 1,000 files, omitting 63 files. For this analysis, we will only use data from these 1,000 files.***



<br>


# Standardizing Data and Combining All Datasets

Upon further investigation, we need to rid the data frame of unnecessary observations and irrelevant variables. The original data included observations from *American Samoa*, *Diamond Princess*, *Grand Princess*, *Guam*, *Northern Mariana Islands*, *Puerto Rico*, *Virgin Islands* and *District of Columbia* **(the cruise ships which originally brought the virus to the west)**, which we are not interested in. We are only interested in the 50 United States of America. Several variables in the dataset were reviewed and determined to be unsuitable for our analysis, such as **"FIPS"**, **"Total_Test_Results"**, **"People_Hospitalized"**, **"Case_Fatality_Ratio"**, **"UID"**, **"ISO3"**, **"Testing_Rate"**, **"Hospitalization_Rate"**, **"People_Tested"**, **"Mortality_Rate"**, **"Date"**, **"Incident_Rate"**. Further, we address missing values in the active cases and recovered cases columns, adding appropriate values based on the data.
```{r, echo = FALSE, eval = FALSE}
detach("package:MASS", unload = TRUE)
library(dplyr)
# Defining the banned strings and pattern for row removal
ban_list <- c("American Samoa", "Diamond Princess", "Grand Princess", "Guam", 
              "Northern Mariana Islands", "Puerto Rico", "Virgin Islands",
              "District of Columbia", "Recovered")
# Creating a regex pattern (the matching will be case-insensitive)
ban_pattern <- paste(ban_list, collapse = "|")

# Defining the column names to remove (case-insensitive)
cols_to_remove <- c("FIPS", "Total_Test_Results", "People_Hospitalized",
                    "Case_Fatality_Ratio", "UID", "ISO3", "Testing_Rate", "Hospitalization_Rate",
                    "People_Tested", "Mortality_Rate", "Date","Incident_Rate")

# Processing each data frame in covid_data_list
covid_data_list_clean <- lapply(covid_data_list, function(df) {
  
  # Removing rows based on banned values in columns whose name contains "State" or "Province"
  # Identifying columns to check (ignoring case)
  cols_to_check <- names(df)[str_detect(names(df), regex("state|province", ignore_case = TRUE))]
  
  if (length(cols_to_check) > 0) {
    # Filtering out any row where at least one of the checked columns contains a banned string
    df <- df %>% 
      filter(!if_any(all_of(cols_to_check), ~ str_detect(.x, regex(ban_pattern, ignore_case = TRUE))))
  }
  
  # Removing unwanted columns (ignoring case)
  # We compare the lower-case version of each column name
  df <- df %>% 
    select(-which(tolower(names(.)) %in% tolower(cols_to_remove)))
  
  # Replacing missing values in "Active" with the corresponding value from "Confirmed"
  # Locating the columns regardless of case
  active_col <- names(df)[tolower(names(df)) == "active"]
  confirmed_col <- names(df)[tolower(names(df)) == "confirmed"]
  
  if (length(active_col) == 1 && length(confirmed_col) == 1) {
    # Replacing if NA or blank ("")
    df[[active_col]] <- ifelse(is.na(df[[active_col]]) | df[[active_col]] == "",
                               df[[confirmed_col]],
                               df[[active_col]])
  }
  
  # Replacing missing values in "Recovered" with 0
  recovered_col <- names(df)[tolower(names(df)) == "recovered"]
  
  if (length(recovered_col) == 1) {
    df[[recovered_col]] <- ifelse(is.na(df[[recovered_col]]) | df[[recovered_col]] == "",
                                  0,
                                  df[[recovered_col]])
  }
  
  return(df)
})
```

<br>
Daily Covid Dataset Before:
```{r, echo=FALSE}
example_1 <- read.csv("C:/Users/William Roche/Downloads/School/MSDA/13 STAT 6382/P2/Data/COVID-19-master/COVID-19-master/csse_covid_19_data/csse_covid_19_daily_reports_us/03-06-2022.csv", check.names = FALSE)
head(example_1,5)
```

Daily Covid Dataset After:
```{r, echo=FALSE}
example_1_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/covid_df.csv", check.names = FALSE)
head(example_1_5,5)
```


## Verifying Data Standardization Before Merging

After processing the vast quantity of files, we make sure each file consists of 50 rows of data, corresponding to the 50 states in the US, and that they don't include erroneous columns, or extra unnecessary data.
```{r, echo = TRUE, eval = FALSE}
# Checking if all CSVs have the same column names:
# Getting the column names from the first CSV
first_names <- names(covid_data_list_clean[[1]])

# Using purrr::map_lgl to compare each CSV's column names with the first CSV
all_same_columns <- purrr::map_lgl(covid_data_list_clean, ~ identical(names(.x), first_names)) %>% all()

if (all_same_columns) {
  print("All CSV files have the same column names.")
} else {
  print("Not all CSV files have the same column names.")
}

# Checking if each CSV has exactly 50 rows:
all_have_50_rows <- purrr::map_lgl(covid_data_list_clean, ~ nrow(.x) == 50) %>% all()

if (all_have_50_rows) {
  print("All CSV files have exactly 50 rows.")
} else {
  print("Some CSV files do not have exactly 50 rows.")
}
```

<br>

#Combining Data

We combine all daily Covid data recods into a single data set ordered by State and Date.
```{r,echo= TRUE, eval=FALSE}
# Binding all rows of clean covid data set list
covid_df <- bind_rows(covid_data_list_clean)
write_csv(covid_df, file.path(main_dir, "covid_df.csv"))
```

# Weather Data Transformations

A portion of our weather data is saved in 4 folders separated by year, and labeled by state in each respective folder in the working directory. We process each file by creating a new column for the state, specifying the year column, removing granular data and calculating averages in order to smooth some statistics. We make sure the data has a `State` and `Year` column, as these will serve as join keys when combining our multiple data sets.
```{r, echo = TRUE, eval = FALSE}
detach("package:MASS", unload = TRUE)
library(dplyr)
# Defining the main data directory
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data"

# Listing all year folders
year_folders <- c("2020", "2021", "2022", "2023")

# Defining function to process each file
process_weather_file <- function(file_path, year) {
  # Extracting state name from file name (without extension)
  state <- tools::file_path_sans_ext(basename(file_path))
  
  # Loading the data
  df <- read_csv(file_path, show_col_types = FALSE)
  
  # Checking if the "date" column exists, otherwise return empty
  if (!"day" %in% names(df)) return(NULL)
  
  # Removing unnecessary columns
  remove_cols <- c("station", "min_rh", "max_rh")
  df <- df %>%
    select(-all_of(remove_cols)) %>%
    
    # Convert date column to Date format & extract month
    mutate(date = ymd(day),  # Assuming the date is in YYYY-MM-DD format
           Month = month(day)) %>%
    
    # Creating avg_temp column
    mutate(avg_temp = ifelse("min_temp_f" %in% names(df) & "max_temp_f" %in% names(df) &
                             !is.na(min_temp_f) & !is.na(max_temp_f),
                             (min_temp_f + max_temp_f) / 2, NA),
           
           # Creating avg_dp column
           avg_dp = ifelse("min_dewpoint_f" %in% names(df) & "max_dewpoint_f" %in% names(df) &
                           !is.na(min_dewpoint_f) & !is.na(max_dewpoint_f),
                           (min_dewpoint_f + max_dewpoint_f) / 2, NA)) %>%
    
    # Removing min/max temp and dew point columns if they exist
    select(-any_of(c("max_temp_f", "min_temp_f", "max_dewpoint_f", "min_dewpoint_f"))) %>%
    
    # Renaming columns if they exist
    rename_with(~ c("avg_precip", "avg_ws")[match(.x, c("precip_in", "avg_wind_speed_kts"))], 
                .cols = any_of(c("precip_in", "avg_wind_speed_kts"))) %>%
    
    # Adding Year and State column
    mutate(Year = year, State = state)

  # Computing monthly averages
  monthly_avg <- df %>%
    group_by(State, Year, Month) %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop")

  # Computing yearly averages
  yearly_avg <- df %>%
    group_by(State, Year) %>%
    summarise(across(where(is.numeric), ~ mean(.x, na.rm = TRUE)), .groups = "drop") %>%
    rename_with(~ paste0("avg_m_", .x), .cols = -c(State, Year))  # Renaming yearly avg cols

  # Merging yearly averages into monthly dataset
  final_data <- left_join(monthly_avg, yearly_avg, by = c("State", "Year"))

  return(final_data)
}

# Processing all files in each year folder
all_data <- map_dfr(year_folders, function(year) {
  year_path <- file.path(main_dir, year)
  
  # Getting all CSV files in the folder
  csv_files <- list.files(year_path, pattern = "\\.csv$", full.names = TRUE)
  
  # Processing each file
  map_dfr(csv_files, function(file) process_weather_file(file, year))
})
# Saving final combined data set to cut down on computing time
write_csv(all_data, file.path(main_dir, "combined_weather_data.csv"))
```

Weather Dataset Before:
```{r, echo=FALSE}
example_2 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data/2020/Alabama.csv", check.names = FALSE)
head(example_2,5)
```

Weather Dataset After:
```{r, echo=FALSE}
example_2_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Weather Data/combined_weather_data.csv", check.names = FALSE)
head(example_2_5,5)
```



<br>

# Population Data Transformations

We reshape data from wide format to long, keeping the state and area columns the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, eval = FALSE}
# Reading the data
df <- read.csv("./Data/Population.csv")
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"

# Reshaping data from wide to long format for years
population_df <- df %>%
  pivot_longer(cols = 2:5, names_to = "year", values_to = "pop_d") %>%
  mutate(year = sub("^X", "", year))  
# Removing "X" from year values (common when reading year column in csv files, R is reading s a string instead of integer)
write_csv(population_df, file.path(main_dir, "population_df.csv"))
```

Population Dataset Before:
```{r, echo=FALSE}
example_3 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Population.csv", check.names = FALSE)
head(example_3,5)
```

Population Dataset After:
```{r, echo=FALSE}
example_3_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/population_df.csv", check.names = FALSE)
head(example_3_5,5)
```


# Population Age Statistics Data Transformations

From the age statistics by population data, we reshape data from wide format to long, keeping the state column the same, and pivoting `2020`, `2021`, `2022`, `2023` with new column name year, and age percentage columns. We ensure to include`State` and `Year` columns.
```{r, echo = TRUE, eval = FALSE}
# Reading the data
df <- read.csv("./Data/infant_elder_avg_age.csv", check.names = FALSE)
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"

# Renaming columns
df <- df %>%
  rename(state = NAME,
         `2020` = POPEST2020_CIV,
         `2021` = POPEST2021_CIV,
         `2022` = POPEST2022_CIV,
         `2023` = POPEST2023_CIV)

# Reshaping from wide to long format for years
df_long <- df %>%
  pivot_longer(cols = `2020`:`2023`, names_to = "year", values_to = "population") 

# Reshaping 'AGE' from long to wide, using the actual records: "Infant percentage" and "elder percentage"
population2_df <- df_long %>%
  pivot_wider(names_from = AGE, values_from = population) %>%
  rename(pop_infant = `Infant percentage`, pop_elder = `elder percentage`)  # Use exact column names

write_csv(population2_df, file.path(main_dir, "population2_df.csv"))
```
<br>

Population Age Statistics Dataset Before:
```{r, echo=FALSE}
example_4 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/infant_elder_avg_age.csv", check.names = FALSE)
head(example_4,5)
```

Population Age Statistics Dataset After:
```{r, echo=FALSE}
example_4_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/population2_df.csv", check.names = FALSE)
head(example_4_5,5)
```

# Airport Traffic Data Transformations

The airport traffic data, or enplanements per state from 2020-2023 is separated into 4 files corresponding to each respective year. We change the state abbreviation back to the full state name in order to ensure `State` column is consistent across all data frames. The enplanement data has been recorded from multiple states, each with multiple airports, so we group the data by state and aggregate it accordingly.
```{r, echo = TRUE, eval = FALSE}
# Defining the path to the data folder
data_path <- "./Data/airport_traffic"
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"
# Listing years corresponding to the files
years <- 2020:2023

# Creating a named vector for state abbreviation-to-full-name mapping
state_abbr <- setNames(state.name, state.abb)

# Defining function to process each file
process_file <- function(year) {
  file <- file.path(data_path, paste0(year, ".csv"))
  # Renaming columns, converting state abbreviations to full names, and dropping rows where state conversion failed 
  df <- read.csv(file, check.names = FALSE) %>%
    rename(state = STATE, enplanements = Enplanements) %>%
    mutate(state = state_abbr[state], year = year) %>%
    drop_na(state)

  return(df)
}

# Reading and processing all files
df_list <- lapply(years, process_file)

# Combining all data over the years into one data set
traffic_df <- bind_rows(df_list) %>%
  group_by(state, year) %>%
  summarise(enplanements = sum(enplanements, na.rm = TRUE), .groups = "drop")
write_csv(traffic_df, file.path(main_dir, "traffic_df.csv"))
```
<br>

Airport Traffic Dataset Before:
```{r, echo=FALSE}
example_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/airport_traffic/2020.csv", check.names = FALSE)
head(example_5,5)
```

Airport Traffic Dataset After:
```{r, echo=FALSE}
example_5_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/traffic_df.csv", check.names = FALSE)
head(example_5_5,5)
```

# Additional Weather Data Import

Average state precipitation data and temperature data are already separated by `State`, the `Date` column is further decomposed into `Year`, ensuring a common join key, and by `Month`. By decomposing `Date` into granular components, trend, seasonality, and residual noise,  applying rolling averages to smooth short-term fluctuations, we can better understand the underlying patterns that impact our dependent variables, enabling more accurate forecasting and strategic decision-making.   

<br>

# Supplemental Data Import

Importing a few more relative data sets that may help support our hypothesis/analysis.
```{r, echo = TRUE, eval = TRUE}
# Reading the data
main_dir <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"
income_df <- read.csv("./Data/poverty_income/Income.csv", check.names = FALSE)
regressors_df <- read.csv("./Data/Regressors.csv", check.names = FALSE)
weather_df <- read.csv("./Data/Weather Data/combined_weather_data.csv", check.names = FALSE)
population_df <- read.csv("./Data/population_df.csv", check.names = FALSE)
population2_df <- read.csv("./Data/population2_df.csv", check.names = FALSE)
traffic_df <- read.csv("./Data/traffic_df.csv", check.names = FALSE)
covid_df <- read.csv("./Data/covid_df.csv", check.names = FALSE)
```

<br>

Income Dataset:
```{r, echo=FALSE}
example_6 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Income.csv", check.names = FALSE)
head(example_6,5)
```

Additional egressors Dataset:
```{r, echo=FALSE}
example_6_5 <- read.csv("C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data/Regressors.csv", check.names = FALSE)
head(example_6_5,5)
```

# Data Wrangling

We ensure that all data sets are in `data.frame` format rather than `data.table` or any other mixed structures.
```{r, echo = TRUE, eval = TRUE}
# Converting data tables to data frames
covid_df <- as.data.frame(covid_df)
population_df <- as.data.frame(population_df)
population2_df <- as.data.frame(population2_df)
traffic_df <- as.data.frame(traffic_df)

# Making sure all data are in data.frame format
objects <- list(covid_df = covid_df, population_df = population_df, population2_df = population2_df, traffic_df = traffic_df, weather_df = weather_df, income_df = income_df, regressors_df = regressors_df)
sapply(objects, is.data.frame)
#getAnywhere(select)
```

<br>



Verifying that columns align properly, and remove any extraneous data to prepare them for consolidation. When necessary, we also address any inconsistencies to streamline the process.
```{r, echo = TRUE, eval = TRUE}
detach("package:MASS", unload = TRUE)
library(dplyr)
# Renaming and making sure case is constant throughout all data sets for join keys
# Decomposing Date into Year and Month
# Reordering columns as needed
# Making sure variable classes are correct
# Selecting only required columns
covid_df <- covid_df %>%
  mutate(
    Date = as.POSIXct(Last_Update),
    across(c(Confirmed, Deaths, Recovered, Active), as.integer),
    Year = as.integer(year(Date)), 
    Month = as.integer(month(Date))
  ) %>%
  rename(State = names(covid_df)[1]) %>%
  relocate(Year, Month, .after = 1) %>%
  select(State, Year, Month, Date, names(covid_df)[4:8])

# Renaming and making sure case is constant throughout all data sets for join keys
# Making sure variable classes are correct
# Selecting only required columns
population_df <- population_df %>%
  mutate(Year = as.integer(.data[[names(population_df)[2]]])) %>%
  select("State" = names(population_df)[1], Year, names(population_df)[3])

# Selecting only required columns
# Reordering columns as needed
weather_df <- weather_df %>%
  select(State, Year, Month, everything())

# Renaming and making sure case is constant throughout all data sets for join keys
# Selecting only required columns
# Making sure variable classes are correct
population2_df <- population2_df %>%
  mutate(Year = as.integer(.data[[names(population2_df)[2]]])) %>%
  select("State" = names(population2_df)[1], Year, names(population2_df)[3], names(population2_df)[4])

# Renaming and making sure case is constant throughout all data sets for join keys
traffic_df <- traffic_df %>%
  select("State" = names(traffic_df)[1], "Year" = names(traffic_df)[2], names(traffic_df)[3])

# Renaming and making sure case is constant throughout all data sets for join keys
# Making sure variable classes are correct
income_df <- income_df %>%
  mutate(Poverty_Percent = as.numeric(Poverty_Percent), 
         Median_Household_Income = as.integer(gsub(",", "",Median_Household_Income))) %>%
  select("State" = names(income_df)[1], "Year" = names(income_df)[2], names(income_df)[3], names(income_df)[4])
```
<br>


# Joining all data frames


We visualize the variable class type for all variables in our data frames before joining them and double-check that the join key maintains a consistent data type across all data frames to facilitate a smooth and accurate join.
```{r, echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE}
dfs <- list(
  covid_df = covid_df,
  population_df = population_df,
  population2_df = population2_df,
  income_df = income_df,
  weather_df = weather_df,
  traffic_df = traffic_df,
  regressors_df = regressors_df
)

# Creating a structured table
df_classes <- map_dfr(dfs, function(df) {
  tibble(
    Variable = names(df),
    Class = sapply(df, function(x) paste(class(x), collapse = ", "))
  )
}, .id = "DataFrame")

# Heatmap visualization
ggplot(df_classes, aes(x = DataFrame, y = Variable, fill = Class)) +
  geom_tile(color = "white") +
  scale_fill_brewer(palette = "Paired") +
  theme_minimal() +
  labs(title = "Variable Classes Across Data Frames", x = "Data Frame", y = "Variable") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```
*Fig. 1 Shows the join key, `State` + `Year`, maintains a consistent data type across all data frames.*
<br>


# Left Join

First use **left_join** by `State`,`Year`, and `Month`, then we join the rest of the data together before joining to original set by `State` and `Year`.
```{r, echo = TRUE, eval = TRUE}
# Left join using composite key (State + Year)
Data_df1 <- population_df %>%
  left_join(weather_df, by = c("State", "Year")) %>%
  left_join(population2_df, by = c("State", "Year")) %>%
  left_join(traffic_df, by = c("State", "Year")) %>%
  left_join(income_df, by = c("State", "Year")) %>%
  left_join(regressors_df, by = c("State", "Year"))

Data_df1 <- Data_df1 %>%
  select(State, Year, Month, everything())

rolling_avg <- covid_df %>%
  left_join(Data_df1, by = c("State", "Year","Month"))
```
<br>

# Rearrange
We rearrange the columns, grouping and bringing `State`, `Year`, and `Date` to the front and keeping `Confirmed`, `Deaths` and `Recovered` to the back.
```{r, echo = TRUE, eval = TRUE}
Data <- rolling_avg %>%
  mutate(avg_ws_y = avg_m_avg_ws, avg_rh_y = avg_m_avg_rh, avg_temp_y = avg_m_avg_temp, avg_dp_y = avg_m_avg_dp, avg_precip_y = avg_m_avg_precip)

Data <- Data %>%
  select(names(Data)[1:8], names(Data)[10:15], names(Data)[22:36])

#main <- "C:/Users/William Roche/Downloads/School/Portfolio/Covid Project/Data"
# Saving final combined data set to cut down on computing time
#write_csv(Data, file.path(main, "Data.csv"))
```

# Checking for `NULL` values.
```{r, echo = TRUE, eval = FALSE}
sum(is.na(Data))
```
<br>

# Exploratory Analysis

# Plotting variables

```{r, warning = FALSE, message = FALSE}
plot(Data$Date, Data$avg_precip, type = "l", col = "blue",
     xlab = "Date", ylab = "Rain (In.)", main = "Precipitaion")

plot(Data$Date, Data$avg_ws, type = "l", col = "blue",
     xlab = "Date", ylab = "Speed (kts.)", main = "Wind Speed")

plot(Data$Date, Data$avg_temp, type = "l", col = "blue",
     xlab = "Date", ylab = "Temp *C", main = "Temperature")

plot(Data$Date, Data$avg_dp, type = "l", col = "blue",
     xlab = "Date", ylab = "Dew Point *C", main = "Dew point")

plot(Data$Date, Data$Confirmed, type = "l", col = "blue",
     xlab = "Date", ylab = "Confirmed Cases", main = "Confirmed")

plot(Data$Date, Data$Deaths, type = "l", col = "blue",
     xlab = "Date", ylab = "Deaths", main = "Deaths")

plot(Data$Date, Data$Recovered, type = "l", col = "blue",
     xlab = "Date", ylab = "Recovered", main = "Recovered")

```





```{r warning = FALSE, message = FALSE, fig.cap="Fig. 4",echo=FALSE}
Data_num <- Data[, sapply(Data, is.numeric)]
data_cor <- cor(Data_num)

corrplot(data_cor, method = "circle", tl.cex = 0.6, 
         order = "hclust", number.cex = 0.7, diag = FALSE)
```
```{r}
str(Data)
```

*Violin plots*
```{r}
# Select numeric, integer, and double variables, excluding "Year", "Month", "Long_", and "Lat"
df_filtered <- Data %>%
  select(where(is.numeric)) %>%
  select(-c(Year, Month, Long_, Lat))

# Convert to long format for ggplot
df_long_all <- pivot_longer(df_filtered, cols = everything(), names_to = "Variable", values_to = "Value")

# Define groups based on the provided list
variable_groups <- list(
  group_2 = c("avg_ws", "avg_rh", "avg_temp", "avg_dp", "avg_precip"),
  group_3 = c("avg_ws_y","avg_rh_y", "avg_temp_y", "avg_dp_y","avg_precip_y"),
  group_5 = c("Airport_Presence", "Beach_Presence"),
  group_7 = c("Confirmed"),
  group_8 = c("Deaths"),
  group_10 = c("pop_d"),
  group_11 = c("avg_precip"),
  group_12 = c("avg_ws"),
  group_13 = c("avg_rh"),
  group_14 = c("avg_temp"),
  group_15 = c("avg_dp"),
  group_16 = c("pop_infant"),
  group_17 = c("pop_elder"),
  group_18 = c("enplanements"),
  group_19 = c("Poverty_Percent"),
  group_20 = c("Median_Household_Income"),
  group_21 = c("Days_Until_Lockdown"),
  group_22 = c("Distance_To_Nearest_Epicenter"),
  group_23 = c("Airport_Presence"),
  group_24 = c("Beach_Presence"),
  group_25 = c("Number_Of_State_Parks"),
  group_26 = c("avg_temp_y"),
  group_27 = c("avg_precip_y")
)

create_violin_plot <- function(var_list, title) {
  df_subset <- df_long_all %>% filter(Variable %in% var_list)

  ggplot(df_subset, aes(x = Variable, y = Value, fill = Variable)) +
    geom_violin(trim = FALSE, alpha = 0.6) +  
    geom_boxplot(width = 0.1, outlier.shape = NA, aes(fill = Variable), color = "black", alpha = 0.2) +  
    stat_summary(fun.min = min, fun.max = max, geom = "errorbar", width = 0.3, color = "black") +  
    scale_fill_brewer(palette = "Paired") +  
    theme_minimal() +
    labs(title = title, x = "Variable", y = "Values") +
    theme(legend.position = "none")
}
# Generate violin plots for each group
violin_plots <- lapply(names(variable_groups), function(group_name) {
  create_violin_plot(variable_groups[[group_name]], paste("Violin Plot for", group_name))
})
# Print each violin plot
for (p in violin_plots) {
  print(p)
}
```

## Correlation matrix

```{r}
#detach("package:MASS", unload = TRUE)
library(dplyr)
# Convert 'Year' to factor and encode 'Month' cyclically
Dat <- Data %>%
  mutate(Year = as.factor(Year),  # Convert Year to factor
         Month_sin = sin(2 * pi * Month / 12), 
         Month_cos = cos(2 * pi * Month / 12)) %>%
  select(-Year, -Month)  # Remove raw Year & Month
# Keep only numeric columns and exclude Lat & Long_
numeric_data <- Dat %>%
  select(where(is.numeric)) %>%
  select(-Lat, -Long_)  # Explicitly remove Lat and Long_

# Compute Pearson correlation
pearson_cor <- cor(numeric_data, use = "pairwise.complete.obs", method = "pearson")

# Compute Spearman correlation
spearman_cor <- cor(numeric_data, use = "pairwise.complete.obs", method = "spearman")

# Plot both correlation matrices
par(mfrow = c(1,2))  # Set up side-by-side plots

corrplot(pearson_cor, method = "color", tl.cex = 0.7, title = "Pearson Correlation", mar=c(0,0,1,0))
corrplot(spearman_cor, method = "color", tl.cex = 0.7, title = "Spearman Correlation", mar=c(0,0,1,0))

par(mfrow = c(1,1))  # Reset plotting layout
```


```{r}
#Data$log_Confirmed <- log1p(Data$Confirmed)
#Data$log_Deaths <- log1p(Data$Deaths)
#Data$log_pop_d <- log1p(Data$pop_d)
#Data$log_avg_precip <- log1p(Data$avg_precip)
#Data$log_enplanements <- log1p(Data$enplanements)
```


```{r}
#dev.off()  # This resets the current plotting device

# Loop through each numeric variable and plot its ACF
for(i in 1:ncol(numeric_data)) {
  acf(numeric_data[, i], main = paste("ACF of", colnames(numeric_data)[i]),lag.max = 365)
}
```

```{r}
Data$log_Confirmed <- log1p(Data$Confirmed)
Data$log_Deaths <- log1p(Data$Deaths)
Data$log_pop_d <- log1p(Data$pop_d)
Data$log_avg_precip <- log1p(Data$avg_precip)
Data$log_enplanements <- log1p(Data$enplanements)

```



```{r,warning=FALSE,message=FALSE}
# Load required libraries
library(dplyr)
library(tseries)

# Convert the date column to Date class (change 'date' to your date column name)
Data$date <- as.Date(Data$Date)

# 2. Compute monthly and yearly averages
monthly <- Data %>%
  group_by(month = format(date, "%Y-%m")) %>%
  summarise(across(c(avg_temp, avg_dp, avg_rh, avg_ws, avg_precip, log_avg_precip), 
                   ~ mean(.x, na.rm = TRUE)))

# 3. Convert to time series objects
## For monthly data:
start_year <- as.numeric(substr(monthly$month[1], 1, 4))
start_month <- as.numeric(substr(monthly$month[1], 6, 7))
monthly_ts <- ts(monthly[,-1], start = c(start_year, start_month), frequency = 12)

# 5. Plot the time series
# --- Group 1: Temperature and Dew Point Plots (2x2 grid) ---
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot(diff(monthly_ts[, "avg_temp"], lag = 12), 
     main = "Temperature Time Series", xlab = "Time", ylab = "Temperature (°C)")
plot(monthly_ts[, "avg_temp"], 
     main = "Monthly avg_temp", ylab = "avg_temp")
plot(diff(monthly_ts[, "avg_dp"], lag = 12), 
     main = "Dew Point Time Series", xlab = "Time", ylab = "Dew Point (°C)")
plot(monthly_ts[, "avg_dp"], 
     main = "Monthly avg_dp", ylab = "avg_dp")

# --- Group 2: Relative Humidity and Wind Speed Plots (2x2 grid) ---
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot(diff(monthly_ts[, "avg_rh"], lag = 12), 
     main = "Relative Humidity Time Series", xlab = "Time", ylab = "Relative Humidity (%)")
plot(monthly_ts[, "avg_rh"], 
     main = "Monthly avg_rh", ylab = "avg_rh")
plot(diff(monthly_ts[, "avg_ws"], lag = 12), 
     main = "Wind Speed Time Series", xlab = "Time", ylab = "Wind Speed (kts)")
plot(monthly_ts[, "avg_ws"], 
     main = "Monthly avg_ws", ylab = "avg_ws")

# --- Group 3: Precipitation Plots (2x1 grid) ---
par(mfrow = c(2, 2), mar = c(4, 4, 2, 1))
plot(diff(monthly_ts[, "avg_precip"], lag = 12), 
     main = "Precipitation Time Series", xlab = "Time", ylab = "Precipitation (ins)")
plot(monthly_ts[, "avg_precip"], 
     main = "Monthly avg_precip", ylab = "avg_precip")
plot(diff(monthly_ts[, "log_avg_precip"], lag = 12), 
     main = "Log_Precipitation Time Series", xlab = "Time", ylab = "Precipitation (ins)")
plot(monthly_ts[, "log_avg_precip"], 
     main = "Monthly log_avg_precip", ylab = "log_avg_precip")
```


```{r,warning=FALSE,message=FALSE}
# Load required libraries
library(dplyr)
library(tseries)


# Convert the date column to Date class 
# (change 'date' to the actual name of your date column if needed)
Data$date <- as.Date(Data$Date)

# Compute monthly averages
monthly <- Data %>%
  group_by(month = format(date, "%Y-%m")) %>%
  summarise(across(c(avg_temp, avg_dp, avg_rh, avg_ws, avg_precip), 
                   ~ mean(.x, na.rm = TRUE)))

# Convert month strings to Date objects (using first day of each month)
monthly$month_date <- as.Date(paste0(monthly$month, "-01"))

# Run ADF tests for monthly aggregated data
adf_results_monthly <- lapply(monthly[, c("avg_temp", "avg_dp", "avg_rh", "avg_ws", "avg_precip")], function(x) {
  adf.test(x)
})


# Print ADF test results
cat("Monthly ADF Test Results:\n")
print(adf_results_monthly)

```




```{r}
library(dplyr)
Data <- Data %>%
  arrange(Month) %>%
  mutate(diff_avg_ws = avg_ws - lag(avg_ws, 12), diff_avg_rh = avg_rh - lag(avg_rh, 12), diff_avg_temp = avg_temp - lag(avg_temp, 12), diff_avg_dp = avg_dp - lag(avg_dp, 12), diff_avg_precip = avg_precip - lag(avg_precip, 12))

Data2 <- Data %>%
  select(State, Date, "y1" = log_Confirmed,"x1" = log_pop_d, "x2"= log_avg_precip, "x3" = diff_avg_ws, "x4" = diff_avg_rh, "x5" = diff_avg_temp, "x6" = diff_avg_dp, "x7" = pop_infant, "x8" = pop_elder, "x9" = log_enplanements, "x10" = Poverty_Percent, "x11" = Median_Household_Income, "x12" = Days_Until_Lockdown, "x13" = Distance_To_Nearest_Epicenter, "x14" = Airport_Presence, "x15" = Beach_Presence, "x16" = Number_Of_State_Parks)

summary(Data2)
```


```{r}
Data2 <- Data2 %>%
  arrange(Date)  # sort by date if not already

# Convert your data into a ts object. Suppose your earliest data is 2020-04, 
# and your data is monthly (freq=12). Adjust the start date as appropriate:
start_year <- 2020
start_month <- 4
monthly_frequency <- 12

y_ts <- ts(Data2$y1,
           start = c(start_year, start_month),
           frequency = monthly_frequency)

# For exogenous variables, we create a matrix (or data frame) of x's
xreg_matrix <- cbind(
  Data2$x1,  # log_pop_d
  Data2$x2,  # log_avg_precip
  Data2$x3,  # diff_avg_ws
  Data2$x4,  # diff_avg_rh
  Data2$x5,  # temperature data
  Data2$x6,  # diff_avg_dp
  Data2$x7,  # pop_infant
  Data2$x8,  # pop_elder
  Data2$x9,  # log_enplanements
  Data2$x10, # Poverty_Percent
  Data2$x11, # Median_Household_Income
  Data2$x12, # Days_Until_Lockdown
  Data2$x13, # Distance_To_Nearest_Epicenter
  Data2$x14, # Airport_Presence
  Data2$x15, # Beach_Presence
  Data2$x16  # Number_Of_State_Parks
)

library(forecast)

# Fit SARIMA with exogenous regressors
fit_sarima <- auto.arima(y_ts,
                         xreg = xreg_matrix,
                         seasonal = TRUE,  # let it detect seasonal terms
                         stepwise = FALSE, # optional, can improve search
                         approximation = FALSE) # optional

summary(fit_sarima)
coef(fit_sarima) 
```


```{r, echo = FALSE, warning = FALSE, message = FALSE}
coef(fit_sarima)             # to list all estimated coefficients
#vif(model1)
```

```{r, messages = FALSE, warning = FALSE}
D_1 <- Data_2 %>%
  select(y1, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x24, x26, x27)

# Create the predictor matrix and response vector
X <- as.matrix(D_1 %>% select(-y1))
y <- D_1$y1

# Optionally, scale your predictors:
X_scaled <- scale(X)

# Fit a Lasso regression using glmnet (alpha = 1 indicates Lasso)
#install.packages("glmnet")
library(glmnet)
cv_lasso <- cv.glmnet(X_scaled, y, alpha = 1)
optimal_lambda <- cv_lasso$lambda.min

# Fit the final Lasso model
final_model <- glmnet(X_scaled, y, alpha = 1, lambda = optimal_lambda)
print(coef(final_model))


```


# Configuring all possible regression combinations

In order to efficiently test each model based on our variables, that will be 2^27-1 models. Computing in R is not feasible, so we will partition our data to load into Apache Spark in order to take advantage of parallel processing power of Apache cutting out computing time considerably.
```{r, echo = FALSE, warning = FALSE, message = FALSE}
#k <- ols_step_all_possible(model_1)
#bestmodel <- k%>%
#  filter(rsquare >= 0.5) %>%
#  filter(cp <= 5) %>%
#  arrange(cp) %>%
#  head(1)

#bestmodel
```

```{r,warning=FALSE,message=FALSE}
# Load the plm package for panel data analysis
install.packages("plm")
library(plm)

# Assume your data frame 'my_data' has a cross-sectional ID (e.g., "id") 
# and a time variable (e.g., "time")
Data$Date <- as.Date(Data$Date)
Data$State <- as.factor(Data$State)

panel_data <- pdata.frame(Data, index = c("State", "Date"))

# Pooled OLS model
pooled_model <- plm(y ~ x1 + x2, data = panel_data, model = "pooling")
summary(pooled_model)
y1, x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21, x24, x26, x27
```

```{r}


```

```{r}


```

```{r}


```

```{r}


```






## Variables List
Median Household Income	Days_Until_Lockdown	Distance_To_Nearest_Epicenter	Airport_Presence	Beach_Presence	Number_Of_State_Parks

$y_1$ *(Dependent Variable) = Daily Deaths Reported*

$y_2$ *(Dependent Variable) = Daily Cases Reported*

$y_3$ *(Dependent Variable) = Daily Recovered Reported*

$x_1$ *(Independent Variable) = Average Precipitation per Year*

$x_2$ *(Independent Variable) = Average Wind Speed per Year*

$x_3$ *(Independent Variable) = Average Relative Humidity per Year*

$x_4$ *(Independent Variable) = Average Dew Point per Year*

$x_5$ *(Independent Variable) = Average Temperature per Year*

$x_6$ *(Independent Variable) = Total Population per Year*

$x_7$ *(Independent Variable) = Total Population Density (p/mi^2 | person per square mile) per Year*

$x_{8}$ *(Independent Variable) = Elder population 65 >= %*

$x_{9}$ *(Independent Variable) = Infant population 5 <= %*

$x_8$ *(Independent Variable) = Number of days between first US case and lock down/shelter-in-place order*

$x_9$ *(Independent Variable) = Distance from capital to the closest of the 5 states with the first case*

$x_{10}$ *(Independent Variable) = If the state has an airport*

$x_{15}$ *(Independent Variable) = If the state has a beach*

$x_{16}$ *(Independent Variable) = Number of state parks*

$x_6$ *(Independent Variable) = State Expenditures for the Fiscal Year*

$x_{17}$ *(Independent Variable) = Airport Traffic*

$x_{20}$ *(Independent Variable) = Poverty percentage*

$x_{21}$ *(Independent Variable) = Unemployment percentage*

$x_{22}$ *(Independent Variable) = Median household income*

$x_{23}$ *(Independent Variable) = deaths/cases*


# Sources

Main Covid19 Data
1. https://github.com/CSSEGISandData/COVID-19/tree/master/csse_covid_19_data/csse_covid_19_daily_reports_us

Precipitation & Temperature Data
2. https://www.ncei.noaa.gov/access/monitoring/climate-at-a-glance/statewide/time-series

Population & Birth Rate Data
3. https://www.census.gov/data/tables/time-series/demo/popest/2020s-state-total.html

Land Area Data
4. https://github.com/jakevdp/data-USstates/blob/master/state-areas.csv

Relative Humidity and Average Dewpoint Data
5. https://www.ncei.noaa.gov/cdo-web/

Uninsured Rates Data
6. https://www.shadac.org/news/2023-acs-tables-state-and-county-uninsured-rates-comparison-year-2022

State Expenditures Data
7. https://nasbo.org/commerce/datasets

Enplanement Data
8. https://www.faa.gov/airports/planning_capacity/passenger_allcargo_stats/passenger

Any and all Climate Data
9. https://mesonet.agron.iastate.edu/request/daily.phtml?

Poverty and Income Data
10. https://www.census.gov/programs-surveys/saipe/data/datasets.html

Unemployment Rates Data
11. https://www.bls.gov/lau/tables.htm#stateaa



